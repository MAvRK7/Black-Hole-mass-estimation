{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXMLpPpjaJP1",
        "outputId": "a1d60e30-27bb-460a-9f85-bb7e222ce30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.37.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Collecting autograd (from pennylane)\n",
            "  Downloading autograd-1.6.2-py3-none-any.whl.metadata (706 bytes)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.4.0)\n",
            "Collecting pennylane-lightning>=0.37 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.1)\n",
            "Collecting future>=0.15.2 (from autograd->pennylane)\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.7.4)\n",
            "Downloading PennyLane-0.37.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.6.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading autograd-1.6.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, semantic-version, rustworkx, future, autoray, autograd, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autograd-1.6.2 autoray-0.6.12 future-1.0.0 pennylane-0.37.0 pennylane-lightning-0.37.0 rustworkx-0.15.1 semantic-version-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unggSvV0XKM1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pennylane as qml\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "import optax\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount the dataset\n",
        "# Assuming the dataset is available as a CSV file\n",
        "data_path = \"/content/typeII_AGN_metadata.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Step 2: Target Column and Features\n",
        "target_column = 'log_bh_mass'\n",
        "features_columns = [\n",
        "    'h_beta_flux', 'h_beta_flux_err', 'oiii_5007_flux', 'oiii_5007_flux_err',\n",
        "    'h_alpha_flux', 'h_alpha_flux_err', 'nii_6584_flux', 'nii_6584_flux_err',\n",
        "    'log_stellar_sigma', 'psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i',\n",
        "    'psfMag_z', 'psfMagErr_u', 'psfMagErr_g', 'psfMagErr_r', 'psfMagErr_i',\n",
        "    'psfMagErr_z', 'mendel_logM_p50', 'mendel_logM_p16', 'mendel_logM_p84',\n",
        "    'mendel_logMt_p50', 'mendel_logMt_p16', 'mendel_logMt_p84', 'mendel_logMb_p50',\n",
        "    'mendel_logMb_p16', 'mendel_logMb_p84', 'mendel_logMd_p50', 'mendel_logMd_p16',\n",
        "    'mendel_logMd_p84', 'simard_b_t_g', 'simard_e_b_t_g', 'simard_b_t_r',\n",
        "    'simard_e_b_t_r', 'simard_Rhlg', 'simard_Rhlr', 'simard_Rchl_g',\n",
        "    'simard_Rchl_r', 'simard_Re', 'simard_e_Re', 'simard_e', 'simard_e_e',\n",
        "    'simard_nb', 'simard_e_nb', 'simard_PpS', 'simard_Pn4'\n",
        "]\n",
        "\n",
        "# Step 3: Handle Missing/NaN values by filling with the mean\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df[features_columns].values\n",
        "y = df[target_column].values\n",
        "\n",
        "# Step 4: Normalize the features\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Step 5: PCA for dimensionality reduction to 4 features\n",
        "pca = PCA(n_components=4)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# Split the data into 80:20 train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to JAX arrays\n",
        "X_train = jnp.array(X_train)\n",
        "X_test = jnp.array(X_test)\n",
        "y_train = jnp.array(y_train)\n",
        "y_test = jnp.array(y_test)\n",
        "\n",
        "# Step 6: Define the quantum circuit and the QML model\n",
        "n_wires = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_wires)"
      ],
      "metadata": {
        "id": "5jImJdr5ZNVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@qml.qnode(dev, interface='jax')\n",
        "def circuit(data, weights):\n",
        "    \"\"\"Quantum circuit ansatz\"\"\"\n",
        "    for i in range(n_wires):\n",
        "        qml.RY(data[i], wires=i)\n",
        "\n",
        "    for i in range(n_wires):\n",
        "        qml.RX(weights[i, 0], wires=i)\n",
        "        qml.RY(weights[i, 1], wires=i)\n",
        "        qml.RX(weights[i, 2], wires=i)\n",
        "        qml.CNOT(wires=[i, (i + 1) % n_wires])\n",
        "\n",
        "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(n_wires)]))\n",
        "\n",
        "def my_model(data, weights, bias):\n",
        "    return circuit(data, weights) + bias\n",
        "\n",
        "# Define loss function\n",
        "@jax.jit\n",
        "def loss_fn(params, data, targets):\n",
        "    predictions = jax.vmap(my_model, in_axes=(0, None, None))(data, params[\"weights\"], params[\"bias\"])\n",
        "    loss = jnp.mean((targets - predictions) ** 2)\n",
        "    return loss\n",
        "\n",
        "# Initialize parameters\n",
        "weights = jnp.ones([n_wires, 3])\n",
        "bias = jnp.array(0.0)\n",
        "params = {\"weights\": weights, \"bias\": bias}\n",
        "\n",
        "# Create the optimizer\n",
        "opt = optax.adam(learning_rate=0.3)\n",
        "opt_state = opt.init(params)\n",
        "\n",
        "# Define the optimization step\n",
        "@jax.jit\n",
        "def update_step(params, opt_state, data, targets):\n",
        "    loss_val, grads = jax.value_and_grad(loss_fn)(params, data, targets)\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss_val"
      ],
      "metadata": {
        "id": "EJQeEBLYbTI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "loss_history = []\n",
        "num_epochs = 250\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    params, opt_state, loss_val = update_step(params, opt_state, X_train, y_train)\n",
        "    loss_history.append(loss_val)\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss_val}\")\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "y_pred_train = jax.vmap(my_model, in_axes=(0, None, None))(X_train, params[\"weights\"], params[\"bias\"])\n",
        "y_pred_test = jax.vmap(my_model, in_axes=(0, None, None))(X_test, params[\"weights\"], params[\"bias\"])\n",
        "\n",
        "\n",
        "r2 = r2_score(y_test, y_pred_test)\n",
        "mae = mean_absolute_error(y_test, y_pred_test)\n",
        "mse = mean_squared_error(y_test, y_pred_test)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate the standard deviation and accuracy by error\n",
        "def calc_std_and_accuracy(y_true, y_pred):\n",
        "    errors = y_true - y_pred\n",
        "    std = np.std(errors)\n",
        "    y_range = np.max(y_true) - np.min(y_true)\n",
        "    accuracy = (1 - np.abs(errors) / y_range) * 100\n",
        "    return std, accuracy\n",
        "\n",
        "# Get the standard deviation and accuracy\n",
        "std, accuracy = calc_std_and_accuracy(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "GL-7kqOybW9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16746bc9-9678-4958-91dd-53abf99f07a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.3773617148399353\n",
            "Epoch: 1, Loss: 0.377393513917923\n",
            "Epoch: 2, Loss: 0.37727296352386475\n",
            "Epoch: 3, Loss: 0.37723076343536377\n",
            "Epoch: 4, Loss: 0.37714648246765137\n",
            "Epoch: 5, Loss: 0.37730419635772705\n",
            "Epoch: 6, Loss: 0.3772878050804138\n",
            "Epoch: 7, Loss: 0.3772973120212555\n",
            "Epoch: 8, Loss: 0.3772176206111908\n",
            "Epoch: 9, Loss: 0.3770988881587982\n",
            "Epoch: 10, Loss: 0.37716954946517944\n",
            "Epoch: 11, Loss: 0.37706074118614197\n",
            "Epoch: 12, Loss: 0.3770756423473358\n",
            "Epoch: 13, Loss: 0.37723517417907715\n",
            "Epoch: 14, Loss: 0.37724682688713074\n",
            "Epoch: 15, Loss: 0.3773193061351776\n",
            "Epoch: 16, Loss: 0.3771543800830841\n",
            "Epoch: 17, Loss: 0.3773341774940491\n",
            "Epoch: 18, Loss: 0.3773270845413208\n",
            "Epoch: 19, Loss: 0.37720823287963867\n",
            "Epoch: 20, Loss: 0.3771560490131378\n",
            "Epoch: 21, Loss: 0.37692710757255554\n",
            "Epoch: 22, Loss: 0.3772311210632324\n",
            "Epoch: 23, Loss: 0.37731263041496277\n",
            "Epoch: 24, Loss: 0.377319872379303\n",
            "Epoch: 25, Loss: 0.37727904319763184\n",
            "Epoch: 26, Loss: 0.3772953450679779\n",
            "Epoch: 27, Loss: 0.37720245122909546\n",
            "Epoch: 28, Loss: 0.3771435618400574\n",
            "Epoch: 29, Loss: 0.37714141607284546\n",
            "Epoch: 30, Loss: 0.37691277265548706\n",
            "Epoch: 31, Loss: 0.37715721130371094\n",
            "Epoch: 32, Loss: 0.3771708905696869\n",
            "Epoch: 33, Loss: 0.3772173523902893\n",
            "Epoch: 34, Loss: 0.37712928652763367\n",
            "Epoch: 35, Loss: 0.37717312574386597\n",
            "Epoch: 36, Loss: 0.3772876262664795\n",
            "Epoch: 37, Loss: 0.3771636188030243\n",
            "Epoch: 38, Loss: 0.3771410882472992\n",
            "Epoch: 39, Loss: 0.3771013617515564\n",
            "Epoch: 40, Loss: 0.3770718276500702\n",
            "Epoch: 41, Loss: 0.377153217792511\n",
            "Epoch: 42, Loss: 0.3772100806236267\n",
            "Epoch: 43, Loss: 0.3771756589412689\n",
            "Epoch: 44, Loss: 0.37714943289756775\n",
            "Epoch: 45, Loss: 0.3771679103374481\n",
            "Epoch: 46, Loss: 0.37718331813812256\n",
            "Epoch: 47, Loss: 0.3770753741264343\n",
            "Epoch: 48, Loss: 0.3770075738430023\n",
            "Epoch: 49, Loss: 0.37707656621932983\n",
            "Epoch: 50, Loss: 0.3769763708114624\n",
            "Epoch: 51, Loss: 0.377150297164917\n",
            "Epoch: 52, Loss: 0.37711548805236816\n",
            "Epoch: 53, Loss: 0.3771093785762787\n",
            "Epoch: 54, Loss: 0.37700673937797546\n",
            "Epoch: 55, Loss: 0.377045601606369\n",
            "Epoch: 56, Loss: 0.37697339057922363\n",
            "Epoch: 57, Loss: 0.377005398273468\n",
            "Epoch: 58, Loss: 0.37697330117225647\n",
            "Epoch: 59, Loss: 0.3770284354686737\n",
            "Epoch: 60, Loss: 0.37705960869789124\n",
            "Epoch: 61, Loss: 0.3768945336341858\n",
            "Epoch: 62, Loss: 0.3771677315235138\n",
            "Epoch: 63, Loss: 0.3768882751464844\n",
            "Epoch: 64, Loss: 0.37691089510917664\n",
            "Epoch: 65, Loss: 0.3771047592163086\n",
            "Epoch: 66, Loss: 0.37690451741218567\n",
            "Epoch: 67, Loss: 0.3768422305583954\n",
            "Epoch: 68, Loss: 0.37694236636161804\n",
            "Epoch: 69, Loss: 0.3770776093006134\n",
            "Epoch: 70, Loss: 0.37707623839378357\n",
            "Epoch: 71, Loss: 0.3770641088485718\n",
            "Epoch: 72, Loss: 0.37704235315322876\n",
            "Epoch: 73, Loss: 0.37704065442085266\n",
            "Epoch: 74, Loss: 0.37712931632995605\n",
            "Epoch: 75, Loss: 0.37697532773017883\n",
            "Epoch: 76, Loss: 0.3769014775753021\n",
            "Epoch: 77, Loss: 0.3770624101161957\n",
            "Epoch: 78, Loss: 0.3771062195301056\n",
            "Epoch: 79, Loss: 0.37707263231277466\n",
            "Epoch: 80, Loss: 0.3771214187145233\n",
            "Epoch: 81, Loss: 0.376921683549881\n",
            "Epoch: 82, Loss: 0.3771559000015259\n",
            "Epoch: 83, Loss: 0.37708261609077454\n",
            "Epoch: 84, Loss: 0.3769958019256592\n",
            "Epoch: 85, Loss: 0.37713366746902466\n",
            "Epoch: 86, Loss: 0.3771832585334778\n",
            "Epoch: 87, Loss: 0.37700575590133667\n",
            "Epoch: 88, Loss: 0.3770884871482849\n",
            "Epoch: 89, Loss: 0.3768349885940552\n",
            "Epoch: 90, Loss: 0.3770534098148346\n",
            "Epoch: 91, Loss: 0.3771668076515198\n",
            "Epoch: 92, Loss: 0.3769446611404419\n",
            "Epoch: 93, Loss: 0.3770633935928345\n",
            "Epoch: 94, Loss: 0.37693890929222107\n",
            "Epoch: 95, Loss: 0.3771117329597473\n",
            "Epoch: 96, Loss: 0.3769364655017853\n",
            "Epoch: 97, Loss: 0.3770715296268463\n",
            "Epoch: 98, Loss: 0.376922070980072\n",
            "Epoch: 99, Loss: 0.37713468074798584\n",
            "Epoch: 100, Loss: 0.3767782747745514\n",
            "Epoch: 101, Loss: 0.3769318163394928\n",
            "Epoch: 102, Loss: 0.37699073553085327\n",
            "Epoch: 103, Loss: 0.3770238161087036\n",
            "Epoch: 104, Loss: 0.3769461512565613\n",
            "Epoch: 105, Loss: 0.37679025530815125\n",
            "Epoch: 106, Loss: 0.3768649399280548\n",
            "Epoch: 107, Loss: 0.3769122362136841\n",
            "Epoch: 108, Loss: 0.37691500782966614\n",
            "Epoch: 109, Loss: 0.37694230675697327\n",
            "Epoch: 110, Loss: 0.37685006856918335\n",
            "Epoch: 111, Loss: 0.37688520550727844\n",
            "Epoch: 112, Loss: 0.3769022226333618\n",
            "Epoch: 113, Loss: 0.3770224153995514\n",
            "Epoch: 114, Loss: 0.37698131799697876\n",
            "Epoch: 115, Loss: 0.3768586218357086\n",
            "Epoch: 116, Loss: 0.37690794467926025\n",
            "Epoch: 117, Loss: 0.3768962025642395\n",
            "Epoch: 118, Loss: 0.3768881559371948\n",
            "Epoch: 119, Loss: 0.3768947422504425\n",
            "Epoch: 120, Loss: 0.3769509494304657\n",
            "Epoch: 121, Loss: 0.3770449459552765\n",
            "Epoch: 122, Loss: 0.37700513005256653\n",
            "Epoch: 123, Loss: 0.37694087624549866\n",
            "Epoch: 124, Loss: 0.376845121383667\n",
            "Epoch: 125, Loss: 0.3769714832305908\n",
            "Epoch: 126, Loss: 0.37693384289741516\n",
            "Epoch: 127, Loss: 0.37690892815589905\n",
            "Epoch: 128, Loss: 0.37691330909729004\n",
            "Epoch: 129, Loss: 0.37692561745643616\n",
            "Epoch: 130, Loss: 0.37687307596206665\n",
            "Epoch: 131, Loss: 0.3767971694469452\n",
            "Epoch: 132, Loss: 0.3768758475780487\n",
            "Epoch: 133, Loss: 0.37692514061927795\n",
            "Epoch: 134, Loss: 0.37690579891204834\n",
            "Epoch: 135, Loss: 0.37703412771224976\n",
            "Epoch: 136, Loss: 0.37689781188964844\n",
            "Epoch: 137, Loss: 0.3769172728061676\n",
            "Epoch: 138, Loss: 0.3769948184490204\n",
            "Epoch: 139, Loss: 0.377044141292572\n",
            "Epoch: 140, Loss: 0.37686455249786377\n",
            "Epoch: 141, Loss: 0.3769618570804596\n",
            "Epoch: 142, Loss: 0.37696573138237\n",
            "Epoch: 143, Loss: 0.37696123123168945\n",
            "Epoch: 144, Loss: 0.3768020272254944\n",
            "Epoch: 145, Loss: 0.3769812285900116\n",
            "Epoch: 146, Loss: 0.376894474029541\n",
            "Epoch: 147, Loss: 0.37689605355262756\n",
            "Epoch: 148, Loss: 0.37696996331214905\n",
            "Epoch: 149, Loss: 0.3770034909248352\n",
            "Epoch: 150, Loss: 0.3769114315509796\n",
            "Epoch: 151, Loss: 0.3767445385456085\n",
            "Epoch: 152, Loss: 0.3768879473209381\n",
            "Epoch: 153, Loss: 0.37691131234169006\n",
            "Epoch: 154, Loss: 0.3768983781337738\n",
            "Epoch: 155, Loss: 0.37691134214401245\n",
            "Epoch: 156, Loss: 0.3769095838069916\n",
            "Epoch: 157, Loss: 0.37700966000556946\n",
            "Epoch: 158, Loss: 0.37683549523353577\n",
            "Epoch: 159, Loss: 0.37686923146247864\n",
            "Epoch: 160, Loss: 0.37690502405166626\n",
            "Epoch: 161, Loss: 0.3769921362400055\n",
            "Epoch: 162, Loss: 0.3769867420196533\n",
            "Epoch: 163, Loss: 0.3770099878311157\n",
            "Epoch: 164, Loss: 0.37697121500968933\n",
            "Epoch: 165, Loss: 0.3768571615219116\n",
            "Epoch: 166, Loss: 0.3768171966075897\n",
            "Epoch: 167, Loss: 0.3767533600330353\n",
            "Epoch: 168, Loss: 0.3768916130065918\n",
            "Epoch: 169, Loss: 0.3770015835762024\n",
            "Epoch: 170, Loss: 0.3769899904727936\n",
            "Epoch: 171, Loss: 0.3767395317554474\n",
            "Epoch: 172, Loss: 0.3768174648284912\n",
            "Epoch: 173, Loss: 0.3769020438194275\n",
            "Epoch: 174, Loss: 0.37686654925346375\n",
            "Epoch: 175, Loss: 0.3769199550151825\n",
            "Epoch: 176, Loss: 0.3769213557243347\n",
            "Epoch: 177, Loss: 0.3768075406551361\n",
            "Epoch: 178, Loss: 0.3768988847732544\n",
            "Epoch: 179, Loss: 0.3768932819366455\n",
            "Epoch: 180, Loss: 0.37689101696014404\n",
            "Epoch: 181, Loss: 0.3768748641014099\n",
            "Epoch: 182, Loss: 0.3770785927772522\n",
            "Epoch: 183, Loss: 0.37699443101882935\n",
            "Epoch: 184, Loss: 0.3768295347690582\n",
            "Epoch: 185, Loss: 0.3768792450428009\n",
            "Epoch: 186, Loss: 0.3767925500869751\n",
            "Epoch: 187, Loss: 0.3769797384738922\n",
            "Epoch: 188, Loss: 0.3769987225532532\n",
            "Epoch: 189, Loss: 0.3768101632595062\n",
            "Epoch: 190, Loss: 0.376676082611084\n",
            "Epoch: 191, Loss: 0.3768073618412018\n",
            "Epoch: 192, Loss: 0.37690696120262146\n",
            "Epoch: 193, Loss: 0.37689220905303955\n",
            "Epoch: 194, Loss: 0.37684789299964905\n",
            "Epoch: 195, Loss: 0.3768112063407898\n",
            "Epoch: 196, Loss: 0.376729816198349\n",
            "Epoch: 197, Loss: 0.37688300013542175\n",
            "Epoch: 198, Loss: 0.3768373429775238\n",
            "Epoch: 199, Loss: 0.37690070271492004\n",
            "Epoch: 200, Loss: 0.37674403190612793\n",
            "Epoch: 201, Loss: 0.37693971395492554\n",
            "Epoch: 202, Loss: 0.37685465812683105\n",
            "Epoch: 203, Loss: 0.3768210709095001\n",
            "Epoch: 204, Loss: 0.3769323229789734\n",
            "Epoch: 205, Loss: 0.3768745958805084\n",
            "Epoch: 206, Loss: 0.3768686354160309\n",
            "Epoch: 207, Loss: 0.37679120898246765\n",
            "Epoch: 208, Loss: 0.3768966495990753\n",
            "Epoch: 209, Loss: 0.37692752480506897\n",
            "Epoch: 210, Loss: 0.3768560290336609\n",
            "Epoch: 211, Loss: 0.37683767080307007\n",
            "Epoch: 212, Loss: 0.37682947516441345\n",
            "Epoch: 213, Loss: 0.37680646777153015\n",
            "Epoch: 214, Loss: 0.376778781414032\n",
            "Epoch: 215, Loss: 0.3770121932029724\n",
            "Epoch: 216, Loss: 0.3768133819103241\n",
            "Epoch: 217, Loss: 0.37679317593574524\n",
            "Epoch: 218, Loss: 0.3767280578613281\n",
            "Epoch: 219, Loss: 0.3768009841442108\n",
            "Epoch: 220, Loss: 0.3768717348575592\n",
            "Epoch: 221, Loss: 0.37692275643348694\n",
            "Epoch: 222, Loss: 0.3769593834877014\n",
            "Epoch: 223, Loss: 0.37686216831207275\n",
            "Epoch: 224, Loss: 0.376830518245697\n",
            "Epoch: 225, Loss: 0.37693020701408386\n",
            "Epoch: 226, Loss: 0.37673136591911316\n",
            "Epoch: 227, Loss: 0.3767983913421631\n",
            "Epoch: 228, Loss: 0.3767569661140442\n",
            "Epoch: 229, Loss: 0.3768313527107239\n",
            "Epoch: 230, Loss: 0.37685009837150574\n",
            "Epoch: 231, Loss: 0.37682169675827026\n",
            "Epoch: 232, Loss: 0.3767639994621277\n",
            "Epoch: 233, Loss: 0.3767281472682953\n",
            "Epoch: 234, Loss: 0.37676045298576355\n",
            "Epoch: 235, Loss: 0.3768366873264313\n",
            "Epoch: 236, Loss: 0.37675800919532776\n",
            "Epoch: 237, Loss: 0.37683364748954773\n",
            "Epoch: 238, Loss: 0.3768010437488556\n",
            "Epoch: 239, Loss: 0.37689077854156494\n",
            "Epoch: 240, Loss: 0.37684547901153564\n",
            "Epoch: 241, Loss: 0.37696725130081177\n",
            "Epoch: 242, Loss: 0.3769156038761139\n",
            "Epoch: 243, Loss: 0.37687361240386963\n",
            "Epoch: 244, Loss: 0.37689119577407837\n",
            "Epoch: 245, Loss: 0.37682202458381653\n",
            "Epoch: 246, Loss: 0.3769438564777374\n",
            "Epoch: 247, Loss: 0.3766952157020569\n",
            "Epoch: 248, Loss: 0.37672409415245056\n",
            "Epoch: 249, Loss: 0.3767123818397522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the error metrics\n",
        "\n",
        "print(f\"R^2: {r2:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"Standard Deviation of Errors: {std:.4f}\")\n",
        "\n",
        "# Calculate the range of y_train\n",
        "y_train_range = np.max(y_train) - np.min(y_train)\n",
        "\n",
        "# Calculate accuracy by MAE, MSE, and RMSE\n",
        "accuracy_by_mae = (1 - mae / y_train_range) * 100\n",
        "accuracy_by_mse = (1 - mse / y_train_range) * 100\n",
        "accuracy_by_rmse = (1 - rmse / y_train_range) * 100\n",
        "\n",
        "# Print the accuracy metrics\n",
        "print(f\"Accuracy by MAE: {accuracy_by_mae:.2f}%\")\n",
        "print(f\"Accuracy by MSE: {accuracy_by_mse:.2f}%\")\n",
        "print(f\"Accuracy by RMSE: {accuracy_by_rmse:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BjFinqFbbbQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "540fb73f-0e04-4088-898b-0298a4abca3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2: 0.0824\n",
            "MAE: 0.4817\n",
            "MSE: 0.3715\n",
            "RMSE: 0.6095\n",
            "Standard Deviation of Errors: 0.6094\n",
            "Accuracy by MAE: 90.45%\n",
            "Accuracy by MSE: 92.63%\n",
            "Accuracy by RMSE: 87.91%\n"
          ]
        }
      ]
    }
  ]
}