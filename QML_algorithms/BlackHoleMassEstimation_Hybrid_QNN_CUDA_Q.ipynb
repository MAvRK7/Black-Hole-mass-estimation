{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install the relevant packages.\n",
        "\n",
        "!pip install matplotlib==3.8.4\n",
        "!pip install torch==2.2.2\n",
        "!pip install torchvision==0.17.0\n",
        "!pip install scikit-learn==1.4.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SdVF8mO8fcXU",
        "outputId": "72858951-82bf-4831-f102-330318a02c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib==3.8.4\n",
            "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.4) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.4) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.4) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.4) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.4) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.4) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.4) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.4) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.4) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.4) (1.16.0)\n",
            "Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "Successfully installed matplotlib-3.8.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "839456e152e14713b587805ddb7c9b36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m500.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m457.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cpu\n",
            "    Uninstalling torch-2.3.0+cpu:\n",
            "      Successfully uninstalled torch-2.3.0+cpu\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cpu requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.18.0+cpu requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\n",
            "Collecting torchvision==0.17.0\n",
            "  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0) (2.31.0)\n",
            "Collecting torch==2.2.0 (from torchvision==0.17.0)\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchvision==0.17.0) (12.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->torchvision==0.17.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->torchvision==0.17.0) (1.3.0)\n",
            "Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m788.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.2\n",
            "    Uninstalling torch-2.2.2:\n",
            "      Successfully uninstalled torch-2.2.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.0+cpu\n",
            "    Uninstalling torchvision-0.18.0+cpu:\n",
            "      Successfully uninstalled torchvision-0.18.0+cpu\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cpu requires torch==2.3.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.2.0 torchvision-0.17.0\n",
            "Collecting scikit-learn==1.4.2\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (3.5.0)\n",
            "Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.3.2\n",
            "    Uninstalling scikit-learn-1.3.2:\n",
            "      Successfully uninstalled scikit-learn-1.3.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cuda-quantum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7xAuOYwflJL",
        "outputId": "45e2243c-d154-4220-b0e2-7bffa707461a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cuda-quantum\n",
            "  Downloading cuda_quantum-0.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting astpretty~=3.0 (from cuda-quantum)\n",
            "  Downloading astpretty-3.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting cuquantum-cu11~=23.10 (from cuda-quantum)\n",
            "  Downloading cuquantum_cu11-23.10.0-py3-none-manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting graphlib-backport>=1.0 (from cuda-quantum)\n",
            "  Downloading graphlib_backport-1.1.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from cuda-quantum) (1.26.4)\n",
            "Collecting custatevec-cu11==1.5.0 (from cuquantum-cu11~=23.10->cuda-quantum)\n",
            "  Downloading custatevec_cu11-1.5.0-py3-none-manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cutensornet-cu11==2.3.0 (from cuquantum-cu11~=23.10->cuda-quantum)\n",
            "  Downloading cutensornet_cu11-2.3.0-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting cutensor-cu11<2,>=1.6.1 (from cutensornet-cu11==2.3.0->cuquantum-cu11~=23.10->cuda-quantum)\n",
            "  Downloading cutensor_cu11-1.7.0-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Downloading cuda_quantum-0.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (105.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.8/105.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astpretty-3.0.0-py2.py3-none-any.whl (4.9 kB)\n",
            "Downloading cuquantum_cu11-23.10.0-py3-none-manylinux2014_x86_64.whl (7.0 kB)\n",
            "Downloading custatevec_cu11-1.5.0-py3-none-manylinux2014_x86_64.whl (38.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cutensornet_cu11-2.3.0-py3-none-manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphlib_backport-1.1.0-py3-none-any.whl (7.1 kB)\n",
            "Downloading cutensor_cu11-1.7.0-py3-none-manylinux2014_x86_64.whl (142.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cutensor-cu11, custatevec-cu11, graphlib-backport, cutensornet-cu11, astpretty, cuquantum-cu11, cuda-quantum\n",
            "Successfully installed astpretty-3.0.0 cuda-quantum-0.7.1 cuquantum-cu11-23.10.0 custatevec-cu11-1.5.0 cutensor-cu11-1.7.0 cutensornet-cu11-2.3.0 graphlib-backport-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cudaq\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Function\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vTNjShn6qTbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set CUDAQ and PyTorch to run on either CPU or GPU\n",
        "device = torch.device('cpu')\n",
        "cudaq.set_target(\"qpp-cpu\")\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "maQRkd1AqW84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define quantum operations\n",
        "def ry(theta, qubit):\n",
        "    # Define the rotation about the Y-axis\n",
        "    cudaq.ry(theta, qubit)\n",
        "\n",
        "def rx(theta, qubit):\n",
        "    # Define the rotation about the X-axis\n",
        "    cudaq.rx(theta, qubit)"
      ],
      "metadata": {
        "id": "WY-FzrkPqaDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Quantum Function\n",
        "class QuantumFunction(Function):\n",
        "    def __init__(self, qubit_count: int, hamiltonian: cudaq.SpinOperator):\n",
        "        @cudaq.kernel\n",
        "        def kernel(qubit_count: int, thetas: np.ndarray):\n",
        "            qubits = cudaq.qvector(qubit_count)\n",
        "            ry(thetas[0], qubits[0])\n",
        "            rx(thetas[1], qubits[0])\n",
        "        self.kernel = kernel\n",
        "        self.qubit_count = qubit_count\n",
        "        self.hamiltonian = hamiltonian\n",
        "\n",
        "    def run(self, theta_vals: torch.Tensor) -> torch.Tensor:\n",
        "        # Ensure theta_vals is converted to numpy array for quantum circuit\n",
        "        theta_vals_np = theta_vals.cpu().numpy()\n",
        "        qubit_count = [self.qubit_count for _ in range(theta_vals_np.shape[0])]\n",
        "        results = cudaq.observe(self.kernel, self.hamiltonian, qubit_count, theta_vals_np)\n",
        "        exp_vals = [results[i].expectation() for i in range(len(results))]\n",
        "        return torch.Tensor(exp_vals).to(device)\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, thetas: torch.Tensor, quantum_circuit, shift) -> torch.Tensor:\n",
        "        ctx.shift = shift\n",
        "        ctx.quantum_circuit = quantum_circuit\n",
        "        exp_vals = ctx.quantum_circuit.run(thetas).reshape(-1, 1)\n",
        "        ctx.save_for_backward(thetas, exp_vals)\n",
        "        return exp_vals\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        thetas, _ = ctx.saved_tensors\n",
        "        gradients = torch.zeros(thetas.shape, device=device)\n",
        "        for i in range(thetas.shape[1]):\n",
        "            thetas_plus = thetas.clone()\n",
        "            thetas_plus[:, i] += ctx.shift\n",
        "            exp_vals_plus = ctx.quantum_circuit.run(thetas_plus)\n",
        "            thetas_minus = thetas.clone()\n",
        "            thetas_minus[:, i] -= ctx.shift\n",
        "            exp_vals_minus = ctx.quantum_circuit.run(thetas_minus)\n",
        "            gradients[:, i] = (exp_vals_plus - exp_vals_minus) / (2 * ctx.shift)\n",
        "        return gradients * grad_output, None, None\n",
        "\n",
        "# Define Quantum Layer\n",
        "class QuantumLayer(nn.Module):\n",
        "    def __init__(self, qubit_count: int, hamiltonian, shift: torch.Tensor):\n",
        "        super(QuantumLayer, self).__init__()\n",
        "        self.quantum_circuit = QuantumFunction(qubit_count, hamiltonian)\n",
        "        self.shift = shift\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        return QuantumFunction.apply(input, self.quantum_circuit, self.shift)\n",
        "\n",
        "# Define Hybrid Quantum Neural Network\n",
        "class Hybrid_QNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hybrid_QNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(len(features), 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.quantum = QuantumLayer(qubit_count=2, hamiltonian=cudaq.spin.z(0), shift=torch.tensor(torch.pi / 2))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = self.dropout(x)\n",
        "        return torch.sigmoid(self.quantum(x)).view(-1)"
      ],
      "metadata": {
        "id": "dzIdWbwtsELA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "def load_data(path):\n",
        "    data = pd.read_csv(path)\n",
        "    return data\n",
        "dataset_path = \"/content/typeII_AGN_metadata.csv\"\n",
        "data = load_data(dataset_path)"
      ],
      "metadata": {
        "id": "pMux3Z1oqwvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values by filling with the mean of the column\n",
        "data.fillna(data.mean(), inplace=True)"
      ],
      "metadata": {
        "id": "dPAU3qpiq5Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features and target\n",
        "features = [\n",
        "    'h_beta_flux', 'h_beta_flux_err', 'oiii_5007_flux', 'oiii_5007_flux_err',\n",
        "    'h_alpha_flux', 'h_alpha_flux_err', 'nii_6584_flux', 'nii_6584_flux_err',\n",
        "    'log_stellar_sigma', 'psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i',\n",
        "    'psfMag_z', 'psfMagErr_u', 'psfMagErr_g', 'psfMagErr_r', 'psfMagErr_i',\n",
        "    'psfMagErr_z', 'mendel_logM_p50', 'mendel_logM_p16', 'mendel_logM_p84',\n",
        "    'mendel_logMt_p50', 'mendel_logMt_p16', 'mendel_logMt_p84', 'mendel_logMb_p50',\n",
        "    'mendel_logMb_p16', 'mendel_logMb_p84', 'mendel_logMd_p50', 'mendel_logMd_p16',\n",
        "    'mendel_logMd_p84', 'simard_b_t_g', 'simard_e_b_t_g', 'simard_b_t_r',\n",
        "    'simard_e_b_t_r', 'simard_Rhlg', 'simard_Rhlr', 'simard_Rchl_g', 'simard_Rchl_r',\n",
        "    'simard_Re', 'simard_e_Re', 'simard_e', 'simard_e_e', 'simard_nb',\n",
        "    'simard_e_nb', 'simard_PpS', 'simard_Pn4'\n",
        "]\n",
        "\n",
        "target = 'log_bh_mass'\n",
        "\n",
        "X = data[features].values\n",
        "y = data[target].values"
      ],
      "metadata": {
        "id": "1OHBZPkFq8w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Normalize the features\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "# Normalize X and y\n",
        "X = scaler_X.fit_transform(X)\n",
        "y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()"
      ],
      "metadata": {
        "id": "Te9hngFsgEox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to torch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "y = torch.tensor(y, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "zPjlZTpBrCLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Training\n",
        "model = Hybrid_QNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss().to(device)\n",
        "\n",
        "epochs = 800\n",
        "losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_hat_train = model(X_train)\n",
        "    loss = loss_function(y_hat_train, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store the loss\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Calculate and print the current epoch, loss, and standard deviation of losses\n",
        "    std_loss = np.std(losses)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}, Std Dev: {std_loss}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_hat_test = model(X_test)\n",
        "    test_loss = loss_function(y_hat_test, y_test).item()\n",
        "\n",
        "    r2_score_value = -r2_score(y_test.cpu(), y_hat_test.cpu())\n",
        "    mae_score_value = mean_absolute_error(y_test.cpu(), y_hat_test.cpu())\n",
        "    rmse_score_value = np.sqrt(mean_squared_error(y_test.cpu(), y_hat_test.cpu()))\n",
        "    mse_score_value = mean_squared_error(y_test.cpu(), y_hat_test.cpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNQVBsW0rFfK",
        "outputId": "62fc3de6-5678-4534-b348-60d1ef86a65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800, Loss: 1.534818172454834, Std Dev: 0.0\n",
            "Epoch 2/800, Loss: 1.533292293548584, Std Dev: 0.000762939453125\n",
            "Epoch 3/800, Loss: 1.5310633182525635, Std Dev: 0.0015418447604201957\n",
            "Epoch 4/800, Loss: 1.5284020900726318, Std Dev: 0.0024181333966537912\n",
            "Epoch 5/800, Loss: 1.5247104167938232, Std Dev: 0.0035964485129790424\n",
            "Epoch 6/800, Loss: 1.519359827041626, Std Dev: 0.005280462289018804\n",
            "Epoch 7/800, Loss: 1.513153076171875, Std Dev: 0.007290149490262618\n",
            "Epoch 8/800, Loss: 1.501259446144104, Std Dev: 0.010753257707507718\n",
            "Epoch 9/800, Loss: 1.485920786857605, Std Dev: 0.015506941242183445\n",
            "Epoch 10/800, Loss: 1.465240716934204, Std Dev: 0.021853564526893837\n",
            "Epoch 11/800, Loss: 1.4410146474838257, Std Dev: 0.0295135804511722\n",
            "Epoch 12/800, Loss: 1.4075748920440674, Std Dev: 0.039437304245695984\n",
            "Epoch 13/800, Loss: 1.3660502433776855, Std Dev: 0.05183911159272906\n",
            "Epoch 14/800, Loss: 1.3239635229110718, Std Dev: 0.06552279369067202\n",
            "Epoch 15/800, Loss: 1.2888062000274658, Std Dev: 0.07878514988092603\n",
            "Epoch 16/800, Loss: 1.2654274702072144, Std Dev: 0.09020397409804855\n",
            "Epoch 17/800, Loss: 1.256629467010498, Std Dev: 0.09883671627941899\n",
            "Epoch 18/800, Loss: 1.258514165878296, Std Dev: 0.10469800238553854\n",
            "Epoch 19/800, Loss: 1.2706855535507202, Std Dev: 0.10795774805436584\n",
            "Epoch 20/800, Loss: 1.2648805379867554, Std Dev: 0.11064845966692574\n",
            "Epoch 21/800, Loss: 1.2751928567886353, Std Dev: 0.1119569853804816\n",
            "Epoch 22/800, Loss: 1.2789980173110962, Std Dev: 0.11260669466444127\n",
            "Epoch 23/800, Loss: 1.2763195037841797, Std Dev: 0.11305559470427859\n",
            "Epoch 24/800, Loss: 1.2667236328125, Std Dev: 0.11365653805446563\n",
            "Epoch 25/800, Loss: 1.2592717409133911, Std Dev: 0.11430179136415669\n",
            "Epoch 26/800, Loss: 1.2395528554916382, Std Dev: 0.11554240832322181\n",
            "Epoch 27/800, Loss: 1.2227641344070435, Std Dev: 0.11720064113631874\n",
            "Epoch 28/800, Loss: 1.2053849697113037, Std Dev: 0.11926958066410419\n",
            "Epoch 29/800, Loss: 1.2027242183685303, Std Dev: 0.12101464430521212\n",
            "Epoch 30/800, Loss: 1.2040026187896729, Std Dev: 0.12232812616420752\n",
            "Epoch 31/800, Loss: 1.2098126411437988, Std Dev: 0.12311894970471078\n",
            "Epoch 32/800, Loss: 1.2112482786178589, Std Dev: 0.12364039173139779\n",
            "Epoch 33/800, Loss: 1.2058079242706299, Std Dev: 0.12416512678676285\n",
            "Epoch 34/800, Loss: 1.198960781097412, Std Dev: 0.12474250387998435\n",
            "Epoch 35/800, Loss: 1.2004436254501343, Std Dev: 0.12510616113250306\n",
            "Epoch 36/800, Loss: 1.1791281700134277, Std Dev: 0.12600021276761764\n",
            "Epoch 37/800, Loss: 1.1844843626022339, Std Dev: 0.12653739032081598\n",
            "Epoch 38/800, Loss: 1.1709104776382446, Std Dev: 0.12733889246006422\n",
            "Epoch 39/800, Loss: 1.1722763776779175, Std Dev: 0.12793235903218014\n",
            "Epoch 40/800, Loss: 1.1811065673828125, Std Dev: 0.12814699710129676\n",
            "Epoch 41/800, Loss: 1.172640085220337, Std Dev: 0.12848254053953295\n",
            "Epoch 42/800, Loss: 1.169210433959961, Std Dev: 0.128800885957678\n",
            "Epoch 43/800, Loss: 1.1637492179870605, Std Dev: 0.12915682662546976\n",
            "Epoch 44/800, Loss: 1.1653451919555664, Std Dev: 0.12937514662412317\n",
            "Epoch 45/800, Loss: 1.1558263301849365, Std Dev: 0.1297363272531922\n",
            "Epoch 46/800, Loss: 1.1638448238372803, Std Dev: 0.12982213386629535\n",
            "Epoch 47/800, Loss: 1.1638076305389404, Std Dev: 0.12984278449188533\n",
            "Epoch 48/800, Loss: 1.1666104793548584, Std Dev: 0.12974899951326935\n",
            "Epoch 49/800, Loss: 1.1521300077438354, Std Dev: 0.12990125908584968\n",
            "Epoch 50/800, Loss: 1.1506894826889038, Std Dev: 0.13001911599366275\n",
            "Epoch 51/800, Loss: 1.1557360887527466, Std Dev: 0.12997953892037012\n",
            "Epoch 52/800, Loss: 1.1555668115615845, Std Dev: 0.12989796200817946\n",
            "Epoch 53/800, Loss: 1.1490048170089722, Std Dev: 0.12989561008276615\n",
            "Epoch 54/800, Loss: 1.1451247930526733, Std Dev: 0.12991972175354932\n",
            "Epoch 55/800, Loss: 1.157646894454956, Std Dev: 0.12968610039880707\n",
            "Epoch 56/800, Loss: 1.1480456590652466, Std Dev: 0.1295832380768915\n",
            "Epoch 57/800, Loss: 1.148466944694519, Std Dev: 0.12944060407684865\n",
            "Epoch 58/800, Loss: 1.1498045921325684, Std Dev: 0.12924802691887177\n",
            "Epoch 59/800, Loss: 1.1437188386917114, Std Dev: 0.12912493060474808\n",
            "Epoch 60/800, Loss: 1.1414085626602173, Std Dev: 0.1290096714202272\n",
            "Epoch 61/800, Loss: 1.1420400142669678, Std Dev: 0.12885751316198413\n",
            "Epoch 62/800, Loss: 1.1486550569534302, Std Dev: 0.1285873717312591\n",
            "Epoch 63/800, Loss: 1.1465851068496704, Std Dev: 0.12832910088580796\n",
            "Epoch 64/800, Loss: 1.1389108896255493, Std Dev: 0.12815998398355224\n",
            "Epoch 65/800, Loss: 1.1416937112808228, Std Dev: 0.12793274584291484\n",
            "Epoch 66/800, Loss: 1.1458511352539062, Std Dev: 0.12763607368944893\n",
            "Epoch 67/800, Loss: 1.1352900266647339, Std Dev: 0.1274630912550768\n",
            "Epoch 68/800, Loss: 1.136385202407837, Std Dev: 0.12725809333539936\n",
            "Epoch 69/800, Loss: 1.1365665197372437, Std Dev: 0.1270354532002229\n",
            "Epoch 70/800, Loss: 1.134580135345459, Std Dev: 0.12682351639026385\n",
            "Epoch 71/800, Loss: 1.138109564781189, Std Dev: 0.12655574959912066\n",
            "Epoch 72/800, Loss: 1.138670802116394, Std Dev: 0.12627126738930833\n",
            "Epoch 73/800, Loss: 1.1425038576126099, Std Dev: 0.1259361032160994\n",
            "Epoch 74/800, Loss: 1.1277040243148804, Std Dev: 0.12576067730413534\n",
            "Epoch 75/800, Loss: 1.1269525289535522, Std Dev: 0.12558067181547491\n",
            "Epoch 76/800, Loss: 1.1324315071105957, Std Dev: 0.12532672742844447\n",
            "Epoch 77/800, Loss: 1.1412672996520996, Std Dev: 0.12497406911101352\n",
            "Epoch 78/800, Loss: 1.1330541372299194, Std Dev: 0.1246993632458745\n",
            "Epoch 79/800, Loss: 1.1278218030929565, Std Dev: 0.1244720865218878\n",
            "Epoch 80/800, Loss: 1.1377569437026978, Std Dev: 0.12413779804388958\n",
            "Epoch 81/800, Loss: 1.1277222633361816, Std Dev: 0.12389729231924822\n",
            "Epoch 82/800, Loss: 1.1309858560562134, Std Dev: 0.12361749506202013\n",
            "Epoch 83/800, Loss: 1.1324949264526367, Std Dev: 0.12331831858248274\n",
            "Epoch 84/800, Loss: 1.1250190734863281, Std Dev: 0.12308564032768798\n",
            "Epoch 85/800, Loss: 1.1266560554504395, Std Dev: 0.12283073158367361\n",
            "Epoch 86/800, Loss: 1.1373594999313354, Std Dev: 0.12247686601064935\n",
            "Epoch 87/800, Loss: 1.1293299198150635, Std Dev: 0.1221897218291973\n",
            "Epoch 88/800, Loss: 1.129679560661316, Std Dev: 0.12189601980766804\n",
            "Epoch 89/800, Loss: 1.1267848014831543, Std Dev: 0.1216240789968074\n",
            "Epoch 90/800, Loss: 1.136648416519165, Std Dev: 0.12126876450326027\n",
            "Epoch 91/800, Loss: 1.1291038990020752, Std Dev: 0.1209722990366274\n",
            "Epoch 92/800, Loss: 1.1317963600158691, Std Dev: 0.12065233090300226\n",
            "Epoch 93/800, Loss: 1.12534499168396, Std Dev: 0.12038221214821083\n",
            "Epoch 94/800, Loss: 1.1235918998718262, Std Dev: 0.12012346407813267\n",
            "Epoch 95/800, Loss: 1.1238679885864258, Std Dev: 0.11985941838074525\n",
            "Epoch 96/800, Loss: 1.1224157810211182, Std Dev: 0.11960418737367699\n",
            "Epoch 97/800, Loss: 1.116321325302124, Std Dev: 0.11939548074752954\n",
            "Epoch 98/800, Loss: 1.1276745796203613, Std Dev: 0.11909471075572045\n",
            "Epoch 99/800, Loss: 1.1251827478408813, Std Dev: 0.11881107003650242\n",
            "Epoch 100/800, Loss: 1.125475287437439, Std Dev: 0.11852401123901407\n",
            "Epoch 101/800, Loss: 1.1214064359664917, Std Dev: 0.11826527305286809\n",
            "Epoch 102/800, Loss: 1.1240366697311401, Std Dev: 0.11798585024529351\n",
            "Epoch 103/800, Loss: 1.1173603534698486, Std Dev: 0.11775342741602605\n",
            "Epoch 104/800, Loss: 1.1189099550247192, Std Dev: 0.11750724420514676\n",
            "Epoch 105/800, Loss: 1.119251012802124, Std Dev: 0.11725676845250785\n",
            "Epoch 106/800, Loss: 1.1205447912216187, Std Dev: 0.11699577255917734\n",
            "Epoch 107/800, Loss: 1.127303957939148, Std Dev: 0.11668996504438946\n",
            "Epoch 108/800, Loss: 1.1236189603805542, Std Dev: 0.11640759228043587\n",
            "Epoch 109/800, Loss: 1.1162041425704956, Std Dev: 0.11617376419308308\n",
            "Epoch 110/800, Loss: 1.1156005859375, Std Dev: 0.11594238470306317\n",
            "Epoch 111/800, Loss: 1.1140450239181519, Std Dev: 0.11571990199812021\n",
            "Epoch 112/800, Loss: 1.1166136264801025, Std Dev: 0.11547868195116151\n",
            "Epoch 113/800, Loss: 1.1077724695205688, Std Dev: 0.11529597147661184\n",
            "Epoch 114/800, Loss: 1.113732933998108, Std Dev: 0.11507029328313209\n",
            "Epoch 115/800, Loss: 1.11185622215271, Std Dev: 0.11485542640774009\n",
            "Epoch 116/800, Loss: 1.115105152130127, Std Dev: 0.11461839520596324\n",
            "Epoch 117/800, Loss: 1.110999584197998, Std Dev: 0.11440628745645091\n",
            "Epoch 118/800, Loss: 1.1122984886169434, Std Dev: 0.1141845483295979\n",
            "Epoch 119/800, Loss: 1.1109237670898438, Std Dev: 0.1139702182701199\n",
            "Epoch 120/800, Loss: 1.1197370290756226, Std Dev: 0.11370329971038205\n",
            "Epoch 121/800, Loss: 1.1099963188171387, Std Dev: 0.11349306287673303\n",
            "Epoch 122/800, Loss: 1.1021372079849243, Std Dev: 0.11333134680340469\n",
            "Epoch 123/800, Loss: 1.1093610525131226, Std Dev: 0.11312197602693973\n",
            "Epoch 124/800, Loss: 1.10953950881958, Std Dev: 0.11291051109810815\n",
            "Epoch 125/800, Loss: 1.109771966934204, Std Dev: 0.11269679182047117\n",
            "Epoch 126/800, Loss: 1.1023197174072266, Std Dev: 0.11252683526793236\n",
            "Epoch 127/800, Loss: 1.1065094470977783, Std Dev: 0.11232981209004428\n",
            "Epoch 128/800, Loss: 1.110586404800415, Std Dev: 0.1121087247197752\n",
            "Epoch 129/800, Loss: 1.104102373123169, Std Dev: 0.11192386034642429\n",
            "Epoch 130/800, Loss: 1.1039677858352661, Std Dev: 0.11173849395342192\n",
            "Epoch 131/800, Loss: 1.112087368965149, Std Dev: 0.11150775524357172\n",
            "Epoch 132/800, Loss: 1.0948879718780518, Std Dev: 0.11137472186418892\n",
            "Epoch 133/800, Loss: 1.102418065071106, Std Dev: 0.11119464680712332\n",
            "Epoch 134/800, Loss: 1.1083693504333496, Std Dev: 0.11098133123880177\n",
            "Epoch 135/800, Loss: 1.1046302318572998, Std Dev: 0.11078743245049238\n",
            "Epoch 136/800, Loss: 1.0985904932022095, Std Dev: 0.11062611459386157\n",
            "Epoch 137/800, Loss: 1.0986645221710205, Std Dev: 0.11046292785542725\n",
            "Epoch 138/800, Loss: 1.0986595153808594, Std Dev: 0.11029842467538983\n",
            "Epoch 139/800, Loss: 1.0959410667419434, Std Dev: 0.11014780293085723\n",
            "Epoch 140/800, Loss: 1.1059625148773193, Std Dev: 0.1099429128207438\n",
            "Epoch 141/800, Loss: 1.1027464866638184, Std Dev: 0.10975390463268547\n",
            "Epoch 142/800, Loss: 1.0923306941986084, Std Dev: 0.10962015995285103\n",
            "Epoch 143/800, Loss: 1.1002707481384277, Std Dev: 0.10944205186992599\n",
            "Epoch 144/800, Loss: 1.0938104391098022, Std Dev: 0.1092972036433035\n",
            "Epoch 145/800, Loss: 1.0958843231201172, Std Dev: 0.10913990161099311\n",
            "Epoch 146/800, Loss: 1.1045339107513428, Std Dev: 0.10893896389551783\n",
            "Epoch 147/800, Loss: 1.09949791431427, Std Dev: 0.10876199047194444\n",
            "Epoch 148/800, Loss: 1.0974689722061157, Std Dev: 0.10859448788797745\n",
            "Epoch 149/800, Loss: 1.0888558626174927, Std Dev: 0.10847093163405011\n",
            "Epoch 150/800, Loss: 1.0930696725845337, Std Dev: 0.10832356177753057\n",
            "Epoch 151/800, Loss: 1.0946848392486572, Std Dev: 0.10816702349451507\n",
            "Epoch 152/800, Loss: 1.0944100618362427, Std Dev: 0.10801102130173912\n",
            "Epoch 153/800, Loss: 1.0957916975021362, Std Dev: 0.10784756857646358\n",
            "Epoch 154/800, Loss: 1.0916826725006104, Std Dev: 0.10770347091719756\n",
            "Epoch 155/800, Loss: 1.0910168886184692, Std Dev: 0.10756167917200979\n",
            "Epoch 156/800, Loss: 1.089233636856079, Std Dev: 0.10742772286885284\n",
            "Epoch 157/800, Loss: 1.086765170097351, Std Dev: 0.10730502181186792\n",
            "Epoch 158/800, Loss: 1.0925506353378296, Std Dev: 0.10715282975552491\n",
            "Epoch 159/800, Loss: 1.0889294147491455, Std Dev: 0.1070171821925348\n",
            "Epoch 160/800, Loss: 1.092789649963379, Std Dev: 0.10686246025944264\n",
            "Epoch 161/800, Loss: 1.0872572660446167, Std Dev: 0.10673313076596375\n",
            "Epoch 162/800, Loss: 1.0928133726119995, Std Dev: 0.10657702201585234\n",
            "Epoch 163/800, Loss: 1.0889637470245361, Std Dev: 0.10643793422276572\n",
            "Epoch 164/800, Loss: 1.0918141603469849, Std Dev: 0.10628521979003318\n",
            "Epoch 165/800, Loss: 1.0932437181472778, Std Dev: 0.1061258549960944\n",
            "Epoch 166/800, Loss: 1.086971402168274, Std Dev: 0.1059940305751922\n",
            "Epoch 167/800, Loss: 1.086042881011963, Std Dev: 0.10586564049703816\n",
            "Epoch 168/800, Loss: 1.0845158100128174, Std Dev: 0.10574341800763447\n",
            "Epoch 169/800, Loss: 1.0879219770431519, Std Dev: 0.10560488261529613\n",
            "Epoch 170/800, Loss: 1.0940581560134888, Std Dev: 0.10544001132859192\n",
            "Epoch 171/800, Loss: 1.0905401706695557, Std Dev: 0.10528947238030882\n",
            "Epoch 172/800, Loss: 1.0991008281707764, Std Dev: 0.1051052447635725\n",
            "Epoch 173/800, Loss: 1.0860835313796997, Std Dev: 0.10497330948046255\n",
            "Epoch 174/800, Loss: 1.089191198348999, Std Dev: 0.10482766636246917\n",
            "Epoch 175/800, Loss: 1.094435214996338, Std Dev: 0.10466102249802403\n",
            "Epoch 176/800, Loss: 1.0928189754486084, Std Dev: 0.10450066055803292\n",
            "Epoch 177/800, Loss: 1.087473750114441, Std Dev: 0.1043614848617412\n",
            "Epoch 178/800, Loss: 1.0936647653579712, Std Dev: 0.10419781843393772\n",
            "Epoch 179/800, Loss: 1.0945615768432617, Std Dev: 0.1040309904643533\n",
            "Epoch 180/800, Loss: 1.084673523902893, Std Dev: 0.10390269045498968\n",
            "Epoch 181/800, Loss: 1.0766887664794922, Std Dev: 0.10380827431955438\n",
            "Epoch 182/800, Loss: 1.081357479095459, Std Dev: 0.10369239317146846\n",
            "Epoch 183/800, Loss: 1.0846664905548096, Std Dev: 0.10356224070560482\n",
            "Epoch 184/800, Loss: 1.08295476436615, Std Dev: 0.10343856050875258\n",
            "Epoch 185/800, Loss: 1.089254379272461, Std Dev: 0.10329004687716142\n",
            "Epoch 186/800, Loss: 1.081406593322754, Std Dev: 0.10317185389807645\n",
            "Epoch 187/800, Loss: 1.0829288959503174, Std Dev: 0.10304701501434317\n",
            "Epoch 188/800, Loss: 1.0804306268692017, Std Dev: 0.10293169238086405\n",
            "Epoch 189/800, Loss: 1.0818841457366943, Std Dev: 0.10281002065257164\n",
            "Epoch 190/800, Loss: 1.0874624252319336, Std Dev: 0.10266697663468366\n",
            "Epoch 191/800, Loss: 1.0822361707687378, Std Dev: 0.10254328930269122\n",
            "Epoch 192/800, Loss: 1.0876275300979614, Std Dev: 0.10239944493642171\n",
            "Epoch 193/800, Loss: 1.0722295045852661, Std Dev: 0.1023155173772083\n",
            "Epoch 194/800, Loss: 1.0817357301712036, Std Dev: 0.10219251127012152\n",
            "Epoch 195/800, Loss: 1.0817759037017822, Std Dev: 0.1020690312938911\n",
            "Epoch 196/800, Loss: 1.080682635307312, Std Dev: 0.10194931878154051\n",
            "Epoch 197/800, Loss: 1.0841537714004517, Std Dev: 0.10181667045779334\n",
            "Epoch 198/800, Loss: 1.0844591856002808, Std Dev: 0.10168285052407922\n",
            "Epoch 199/800, Loss: 1.0823174715042114, Std Dev: 0.10155649611614267\n",
            "Epoch 200/800, Loss: 1.080252766609192, Std Dev: 0.10143737035187676\n",
            "Epoch 201/800, Loss: 1.0782203674316406, Std Dev: 0.10132538538226314\n",
            "Epoch 202/800, Loss: 1.077330470085144, Std Dev: 0.10121628902489467\n",
            "Epoch 203/800, Loss: 1.0730780363082886, Std Dev: 0.10112286245792149\n",
            "Epoch 204/800, Loss: 1.079973816871643, Std Dev: 0.10100330247028888\n",
            "Epoch 205/800, Loss: 1.0770066976547241, Std Dev: 0.10089409573788113\n",
            "Epoch 206/800, Loss: 1.079694151878357, Std Dev: 0.10077499083384171\n",
            "Epoch 207/800, Loss: 1.081319808959961, Std Dev: 0.10065014538181401\n",
            "Epoch 208/800, Loss: 1.0716570615768433, Std Dev: 0.10055970498213268\n",
            "Epoch 209/800, Loss: 1.0752487182617188, Std Dev: 0.10045538125980616\n",
            "Epoch 210/800, Loss: 1.0797988176345825, Std Dev: 0.10033491083737588\n",
            "Epoch 211/800, Loss: 1.0810085535049438, Std Dev: 0.1002103293909539\n",
            "Epoch 212/800, Loss: 1.076375126838684, Std Dev: 0.10010122152974733\n",
            "Epoch 213/800, Loss: 1.0804109573364258, Std Dev: 0.09997837042767514\n",
            "Epoch 214/800, Loss: 1.06312096118927, Std Dev: 0.0999179898946898\n",
            "Epoch 215/800, Loss: 1.0787484645843506, Std Dev: 0.09979994114237363\n",
            "Epoch 216/800, Loss: 1.077209234237671, Std Dev: 0.09968682230472847\n",
            "Epoch 217/800, Loss: 1.0765647888183594, Std Dev: 0.09957565645180899\n",
            "Epoch 218/800, Loss: 1.076928973197937, Std Dev: 0.09946311079718806\n",
            "Epoch 219/800, Loss: 1.077040433883667, Std Dev: 0.0993500505683051\n",
            "Epoch 220/800, Loss: 1.0699597597122192, Std Dev: 0.09926070532708108\n",
            "Epoch 221/800, Loss: 1.0694375038146973, Std Dev: 0.09917269172270116\n",
            "Epoch 222/800, Loss: 1.0777264833450317, Std Dev: 0.09905674290243657\n",
            "Epoch 223/800, Loss: 1.073380947113037, Std Dev: 0.09895462120572222\n",
            "Epoch 224/800, Loss: 1.0742918252944946, Std Dev: 0.0988492811799273\n",
            "Epoch 225/800, Loss: 1.0715407133102417, Std Dev: 0.098752715874296\n",
            "Epoch 226/800, Loss: 1.076383113861084, Std Dev: 0.09864037942236048\n",
            "Epoch 227/800, Loss: 1.0766942501068115, Std Dev: 0.09852702324419176\n",
            "Epoch 228/800, Loss: 1.0773074626922607, Std Dev: 0.09841177811578204\n",
            "Epoch 229/800, Loss: 1.0710254907608032, Std Dev: 0.0983160636386897\n",
            "Epoch 230/800, Loss: 1.0783295631408691, Std Dev: 0.09819768349078323\n",
            "Epoch 231/800, Loss: 1.0748625993728638, Std Dev: 0.09808961346900598\n",
            "Epoch 232/800, Loss: 1.0741868019104004, Std Dev: 0.09798350224034012\n",
            "Epoch 233/800, Loss: 1.0712003707885742, Std Dev: 0.09788648946964214\n",
            "Epoch 234/800, Loss: 1.072407603263855, Std Dev: 0.09778550605171892\n",
            "Epoch 235/800, Loss: 1.0798521041870117, Std Dev: 0.09766287059903515\n",
            "Epoch 236/800, Loss: 1.075083613395691, Std Dev: 0.09755379226505352\n",
            "Epoch 237/800, Loss: 1.075798749923706, Std Dev: 0.09744263339171945\n",
            "Epoch 238/800, Loss: 1.066321849822998, Std Dev: 0.09736031845030958\n",
            "Epoch 239/800, Loss: 1.0736186504364014, Std Dev: 0.09725523696349198\n",
            "Epoch 240/800, Loss: 1.0748066902160645, Std Dev: 0.09714669748107214\n",
            "Epoch 241/800, Loss: 1.0742226839065552, Std Dev: 0.09703981151672081\n",
            "Epoch 242/800, Loss: 1.0720303058624268, Std Dev: 0.09693919874437223\n",
            "Epoch 243/800, Loss: 1.073177456855774, Std Dev: 0.09683519858873679\n",
            "Epoch 244/800, Loss: 1.0714575052261353, Std Dev: 0.09673607973373591\n",
            "Epoch 245/800, Loss: 1.0784968137741089, Std Dev: 0.09661765175540225\n",
            "Epoch 246/800, Loss: 1.0703922510147095, Std Dev: 0.09652154948935869\n",
            "Epoch 247/800, Loss: 1.0685372352600098, Std Dev: 0.09643073784141615\n",
            "Epoch 248/800, Loss: 1.0692129135131836, Std Dev: 0.09633775628605462\n",
            "Epoch 249/800, Loss: 1.0705888271331787, Std Dev: 0.09624068284261653\n",
            "Epoch 250/800, Loss: 1.0838146209716797, Std Dev: 0.09610993663838217\n",
            "Epoch 251/800, Loss: 1.0738950967788696, Std Dev: 0.09600391950186794\n",
            "Epoch 252/800, Loss: 1.0735251903533936, Std Dev: 0.09589893066804435\n",
            "Epoch 253/800, Loss: 1.0657756328582764, Std Dev: 0.09581566543805567\n",
            "Epoch 254/800, Loss: 1.0672218799591064, Std Dev: 0.09572794555594734\n",
            "Epoch 255/800, Loss: 1.075081467628479, Std Dev: 0.09561884769814585\n",
            "Epoch 256/800, Loss: 1.0751564502716064, Std Dev: 0.0955096766082051\n",
            "Epoch 257/800, Loss: 1.0789188146591187, Std Dev: 0.09539150952636766\n",
            "Epoch 258/800, Loss: 1.0709445476531982, Std Dev: 0.09529347251168795\n",
            "Epoch 259/800, Loss: 1.0759003162384033, Std Dev: 0.09518285024962178\n",
            "Epoch 260/800, Loss: 1.0808651447296143, Std Dev: 0.0950608632930011\n",
            "Epoch 261/800, Loss: 1.0681452751159668, Std Dev: 0.09497051032422078\n",
            "Epoch 262/800, Loss: 1.0684932470321655, Std Dev: 0.09487912567558451\n",
            "Epoch 263/800, Loss: 1.070082187652588, Std Dev: 0.0947834779152187\n",
            "Epoch 264/800, Loss: 1.0755048990249634, Std Dev: 0.09467437285371762\n",
            "Epoch 265/800, Loss: 1.0664153099060059, Std Dev: 0.09458849373187415\n",
            "Epoch 266/800, Loss: 1.0692188739776611, Std Dev: 0.09449507509885136\n",
            "Epoch 267/800, Loss: 1.0684616565704346, Std Dev: 0.0944035919817491\n",
            "Epoch 268/800, Loss: 1.0755250453948975, Std Dev: 0.09429479365516488\n",
            "Epoch 269/800, Loss: 1.064249873161316, Std Dev: 0.09421451715305836\n",
            "Epoch 270/800, Loss: 1.0667357444763184, Std Dev: 0.09412743284335873\n",
            "Epoch 271/800, Loss: 1.0804799795150757, Std Dev: 0.09400818560258374\n",
            "Epoch 272/800, Loss: 1.065395474433899, Std Dev: 0.09392466989860747\n",
            "Epoch 273/800, Loss: 1.0720734596252441, Std Dev: 0.09382438322721677\n",
            "Epoch 274/800, Loss: 1.068732738494873, Std Dev: 0.09373226540870618\n",
            "Epoch 275/800, Loss: 1.0660032033920288, Std Dev: 0.0936470217441825\n",
            "Epoch 276/800, Loss: 1.0725300312042236, Std Dev: 0.09354586143871454\n",
            "Epoch 277/800, Loss: 1.0771926641464233, Std Dev: 0.09343458237941711\n",
            "Epoch 278/800, Loss: 1.0674145221710205, Std Dev: 0.09334588022592967\n",
            "Epoch 279/800, Loss: 1.069333553314209, Std Dev: 0.09325252164828822\n",
            "Epoch 280/800, Loss: 1.0725961923599243, Std Dev: 0.09315169031813529\n",
            "Epoch 281/800, Loss: 1.068847894668579, Std Dev: 0.09305962343283992\n",
            "Epoch 282/800, Loss: 1.072535514831543, Std Dev: 0.09295916303851273\n",
            "Epoch 283/800, Loss: 1.0742440223693848, Std Dev: 0.0928551390804841\n",
            "Epoch 284/800, Loss: 1.0701552629470825, Std Dev: 0.09276031909447764\n",
            "Epoch 285/800, Loss: 1.0705876350402832, Std Dev: 0.09266461025544756\n",
            "Epoch 286/800, Loss: 1.0716683864593506, Std Dev: 0.0925666079953427\n",
            "Epoch 287/800, Loss: 1.0673840045928955, Std Dev: 0.09247844160603269\n",
            "Epoch 288/800, Loss: 1.0673537254333496, Std Dev: 0.09239036768220642\n",
            "Epoch 289/800, Loss: 1.0739142894744873, Std Dev: 0.09228791521458914\n",
            "Epoch 290/800, Loss: 1.0681843757629395, Std Dev: 0.09219808210919411\n",
            "Epoch 291/800, Loss: 1.0716336965560913, Std Dev: 0.09210072584698638\n",
            "Epoch 292/800, Loss: 1.0697880983352661, Std Dev: 0.0920074932870324\n",
            "Epoch 293/800, Loss: 1.065744400024414, Std Dev: 0.0919234659169908\n",
            "Epoch 294/800, Loss: 1.0698004961013794, Std Dev: 0.09183036367751812\n",
            "Epoch 295/800, Loss: 1.078843593597412, Std Dev: 0.09171943185901124\n",
            "Epoch 296/800, Loss: 1.0688714981079102, Std Dev: 0.09162866196124315\n",
            "Epoch 297/800, Loss: 1.064239263534546, Std Dev: 0.09154838469787989\n",
            "Epoch 298/800, Loss: 1.0798442363739014, Std Dev: 0.09143637320808125\n",
            "Epoch 299/800, Loss: 1.0720309019088745, Std Dev: 0.09133938350035041\n",
            "Epoch 300/800, Loss: 1.0697659254074097, Std Dev: 0.09124721341397089\n",
            "Epoch 301/800, Loss: 1.0647821426391602, Std Dev: 0.09116598833585575\n",
            "Epoch 302/800, Loss: 1.069555401802063, Std Dev: 0.09107444015134517\n",
            "Epoch 303/800, Loss: 1.0736277103424072, Std Dev: 0.0909749038011453\n",
            "Epoch 304/800, Loss: 1.0647281408309937, Std Dev: 0.09089398162345166\n",
            "Epoch 305/800, Loss: 1.068785309791565, Std Dev: 0.09080437636263458\n",
            "Epoch 306/800, Loss: 1.070520281791687, Std Dev: 0.09071136439352126\n",
            "Epoch 307/800, Loss: 1.0655080080032349, Std Dev: 0.09062890155007915\n",
            "Epoch 308/800, Loss: 1.0691097974777222, Std Dev: 0.09053895624726362\n",
            "Epoch 309/800, Loss: 1.0661568641662598, Std Dev: 0.09045523269449274\n",
            "Epoch 310/800, Loss: 1.071661353111267, Std Dev: 0.09036051375426721\n",
            "Epoch 311/800, Loss: 1.0746071338653564, Std Dev: 0.0902605330966889\n",
            "Epoch 312/800, Loss: 1.0728673934936523, Std Dev: 0.09016395401801049\n",
            "Epoch 313/800, Loss: 1.0628741979599, Std Dev: 0.09008771153888437\n",
            "Epoch 314/800, Loss: 1.069638967514038, Std Dev: 0.0899975274659698\n",
            "Epoch 315/800, Loss: 1.0728827714920044, Std Dev: 0.08990141285242047\n",
            "Epoch 316/800, Loss: 1.069506287574768, Std Dev: 0.08981180799675897\n",
            "Epoch 317/800, Loss: 1.0787156820297241, Std Dev: 0.08970620019814095\n",
            "Epoch 318/800, Loss: 1.0730845928192139, Std Dev: 0.08961038010541084\n",
            "Epoch 319/800, Loss: 1.0726722478866577, Std Dev: 0.0895155090202481\n",
            "Epoch 320/800, Loss: 1.0631400346755981, Std Dev: 0.08943936260331682\n",
            "Epoch 321/800, Loss: 1.0704401731491089, Std Dev: 0.08934884629226997\n",
            "Epoch 322/800, Loss: 1.0727217197418213, Std Dev: 0.08925440403334473\n",
            "Epoch 323/800, Loss: 1.0701957941055298, Std Dev: 0.08916469354862178\n",
            "Epoch 324/800, Loss: 1.0759577751159668, Std Dev: 0.08906521231949625\n",
            "Epoch 325/800, Loss: 1.061333179473877, Std Dev: 0.08899332609128299\n",
            "Epoch 326/800, Loss: 1.0652334690093994, Std Dev: 0.08891345926706376\n",
            "Epoch 327/800, Loss: 1.0719767808914185, Std Dev: 0.08882119361466286\n",
            "Epoch 328/800, Loss: 1.0621081590652466, Std Dev: 0.08874781735894662\n",
            "Epoch 329/800, Loss: 1.0707697868347168, Std Dev: 0.08865795978731411\n",
            "Epoch 330/800, Loss: 1.0723029375076294, Std Dev: 0.08856564503843774\n",
            "Epoch 331/800, Loss: 1.0698661804199219, Std Dev: 0.08847774911477721\n",
            "Epoch 332/800, Loss: 1.0692944526672363, Std Dev: 0.08839103167399738\n",
            "Epoch 333/800, Loss: 1.0658352375030518, Std Dev: 0.08831079969958844\n",
            "Epoch 334/800, Loss: 1.0805628299713135, Std Dev: 0.08820666268058067\n",
            "Epoch 335/800, Loss: 1.065222978591919, Std Dev: 0.08812790444507673\n",
            "Epoch 336/800, Loss: 1.0713273286819458, Std Dev: 0.08803836921180401\n",
            "Epoch 337/800, Loss: 1.0743091106414795, Std Dev: 0.08794420304314161\n",
            "Epoch 338/800, Loss: 1.0704624652862549, Std Dev: 0.08785652889531205\n",
            "Epoch 339/800, Loss: 1.079371452331543, Std Dev: 0.08775538579148194\n",
            "Epoch 340/800, Loss: 1.0686901807785034, Std Dev: 0.08767116245330067\n",
            "Epoch 341/800, Loss: 1.0656753778457642, Std Dev: 0.08759244160347436\n",
            "Epoch 342/800, Loss: 1.066347599029541, Std Dev: 0.08751260615872283\n",
            "Epoch 343/800, Loss: 1.0659083127975464, Std Dev: 0.08743367028454985\n",
            "Epoch 344/800, Loss: 1.0620152950286865, Std Dev: 0.08736206327741526\n",
            "Epoch 345/800, Loss: 1.0662016868591309, Std Dev: 0.08728278388103042\n",
            "Epoch 346/800, Loss: 1.0665351152420044, Std Dev: 0.08720303167972958\n",
            "Epoch 347/800, Loss: 1.0591500997543335, Std Dev: 0.08713717600375388\n",
            "Epoch 348/800, Loss: 1.064042568206787, Std Dev: 0.0870620261741007\n",
            "Epoch 349/800, Loss: 1.0609664916992188, Std Dev: 0.08699267129572508\n",
            "Epoch 350/800, Loss: 1.0720902681350708, Std Dev: 0.08690424259258643\n",
            "Epoch 351/800, Loss: 1.0668329000473022, Std Dev: 0.08682450725149977\n",
            "Epoch 352/800, Loss: 1.0685070753097534, Std Dev: 0.08674211599215212\n",
            "Epoch 353/800, Loss: 1.0685077905654907, Std Dev: 0.08665988075310464\n",
            "Epoch 354/800, Loss: 1.0619395971298218, Std Dev: 0.08658915383470624\n",
            "Epoch 355/800, Loss: 1.0645337104797363, Std Dev: 0.08651384931352232\n",
            "Epoch 356/800, Loss: 1.0747262239456177, Std Dev: 0.08642265254608224\n",
            "Epoch 357/800, Loss: 1.065603256225586, Std Dev: 0.08634579321059374\n",
            "Epoch 358/800, Loss: 1.0667729377746582, Std Dev: 0.0862671007192221\n",
            "Epoch 359/800, Loss: 1.0653355121612549, Std Dev: 0.08619093083968428\n",
            "Epoch 360/800, Loss: 1.068317174911499, Std Dev: 0.08611001510221011\n",
            "Epoch 361/800, Loss: 1.0618637800216675, Std Dev: 0.08604008340074565\n",
            "Epoch 362/800, Loss: 1.0743199586868286, Std Dev: 0.08595061186965618\n",
            "Epoch 363/800, Loss: 1.0640599727630615, Std Dev: 0.08587709438999003\n",
            "Epoch 364/800, Loss: 1.0656148195266724, Std Dev: 0.08580108614496297\n",
            "Epoch 365/800, Loss: 1.0665186643600464, Std Dev: 0.0857237367841079\n",
            "Epoch 366/800, Loss: 1.0582268238067627, Std Dev: 0.08566083616360033\n",
            "Epoch 367/800, Loss: 1.0672924518585205, Std Dev: 0.08558247477171334\n",
            "Epoch 368/800, Loss: 1.0616668462753296, Std Dev: 0.08551350733454527\n",
            "Epoch 369/800, Loss: 1.0621445178985596, Std Dev: 0.085443781840551\n",
            "Epoch 370/800, Loss: 1.073559284210205, Std Dev: 0.08535675473803855\n",
            "Epoch 371/800, Loss: 1.0663363933563232, Std Dev: 0.08528042679522127\n",
            "Epoch 372/800, Loss: 1.071698546409607, Std Dev: 0.08519633898800605\n",
            "Epoch 373/800, Loss: 1.0671898126602173, Std Dev: 0.085119005279253\n",
            "Epoch 374/800, Loss: 1.0641812086105347, Std Dev: 0.08504652286178198\n",
            "Epoch 375/800, Loss: 1.0627351999282837, Std Dev: 0.0849764969833765\n",
            "Epoch 376/800, Loss: 1.0573015213012695, Std Dev: 0.08491593550355865\n",
            "Epoch 377/800, Loss: 1.069058895111084, Std Dev: 0.0848363321701515\n",
            "Epoch 378/800, Loss: 1.0625282526016235, Std Dev: 0.08476690458081654\n",
            "Epoch 379/800, Loss: 1.0629191398620605, Std Dev: 0.08469692794697384\n",
            "Epoch 380/800, Loss: 1.065271258354187, Std Dev: 0.08462334841918762\n",
            "Epoch 381/800, Loss: 1.065843939781189, Std Dev: 0.08454902335292075\n",
            "Epoch 382/800, Loss: 1.0635403394699097, Std Dev: 0.08447837418270167\n",
            "Epoch 383/800, Loss: 1.0646418333053589, Std Dev: 0.08440611888747923\n",
            "Epoch 384/800, Loss: 1.0664137601852417, Std Dev: 0.08433132393820497\n",
            "Epoch 385/800, Loss: 1.0682029724121094, Std Dev: 0.08425409612961175\n",
            "Epoch 386/800, Loss: 1.0633503198623657, Std Dev: 0.08418420808864045\n",
            "Epoch 387/800, Loss: 1.0609257221221924, Std Dev: 0.0841182609634762\n",
            "Epoch 388/800, Loss: 1.0716780424118042, Std Dev: 0.0840367955921169\n",
            "Epoch 389/800, Loss: 1.0687812566757202, Std Dev: 0.08395936763383133\n",
            "Epoch 390/800, Loss: 1.058522343635559, Std Dev: 0.08389769004366708\n",
            "Epoch 391/800, Loss: 1.064102053642273, Std Dev: 0.08382721595198284\n",
            "Epoch 392/800, Loss: 1.065390944480896, Std Dev: 0.08375495737508706\n",
            "Epoch 393/800, Loss: 1.0641803741455078, Std Dev: 0.0836846013855536\n",
            "Epoch 394/800, Loss: 1.0697182416915894, Std Dev: 0.08360664548288044\n",
            "Epoch 395/800, Loss: 1.0703593492507935, Std Dev: 0.08352803929270089\n",
            "Epoch 396/800, Loss: 1.0697041749954224, Std Dev: 0.08345046360696663\n",
            "Epoch 397/800, Loss: 1.066107988357544, Std Dev: 0.08337791342253206\n",
            "Epoch 398/800, Loss: 1.062631607055664, Std Dev: 0.08331053656676962\n",
            "Epoch 399/800, Loss: 1.0670024156570435, Std Dev: 0.08323701134102568\n",
            "Epoch 400/800, Loss: 1.061898112297058, Std Dev: 0.08317095436677216\n",
            "Epoch 401/800, Loss: 1.066620945930481, Std Dev: 0.08309821512184387\n",
            "Epoch 402/800, Loss: 1.0635541677474976, Std Dev: 0.08302992333458815\n",
            "Epoch 403/800, Loss: 1.0687490701675415, Std Dev: 0.08295464738213217\n",
            "Epoch 404/800, Loss: 1.0685158967971802, Std Dev: 0.08287983874059814\n",
            "Epoch 405/800, Loss: 1.0705550909042358, Std Dev: 0.08280263722379426\n",
            "Epoch 406/800, Loss: 1.0631160736083984, Std Dev: 0.08273551009241967\n",
            "Epoch 407/800, Loss: 1.0691561698913574, Std Dev: 0.08266036717327573\n",
            "Epoch 408/800, Loss: 1.0648162364959717, Std Dev: 0.0825910978969955\n",
            "Epoch 409/800, Loss: 1.062620997428894, Std Dev: 0.08252503758199942\n",
            "Epoch 410/800, Loss: 1.0696624517440796, Std Dev: 0.08244972652046535\n",
            "Epoch 411/800, Loss: 1.064928650856018, Std Dev: 0.0823806916911971\n",
            "Epoch 412/800, Loss: 1.0678354501724243, Std Dev: 0.08230797355380766\n",
            "Epoch 413/800, Loss: 1.0720610618591309, Std Dev: 0.08223033814340264\n",
            "Epoch 414/800, Loss: 1.063464879989624, Std Dev: 0.08216373132557095\n",
            "Epoch 415/800, Loss: 1.0653936862945557, Std Dev: 0.08209462574236111\n",
            "Epoch 416/800, Loss: 1.0600473880767822, Std Dev: 0.08203311968871947\n",
            "Epoch 417/800, Loss: 1.0599347352981567, Std Dev: 0.08197185029467641\n",
            "Epoch 418/800, Loss: 1.0660258531570435, Std Dev: 0.08190225881162233\n",
            "Epoch 419/800, Loss: 1.0690573453903198, Std Dev: 0.08182904582897756\n",
            "Epoch 420/800, Loss: 1.0701862573623657, Std Dev: 0.08175467316464002\n",
            "Epoch 421/800, Loss: 1.0606576204299927, Std Dev: 0.08169278975860102\n",
            "Epoch 422/800, Loss: 1.0583523511886597, Std Dev: 0.08163434715669157\n",
            "Epoch 423/800, Loss: 1.0586023330688477, Std Dev: 0.08157558765611583\n",
            "Epoch 424/800, Loss: 1.064098834991455, Std Dev: 0.0815092291224547\n",
            "Epoch 425/800, Loss: 1.0734164714813232, Std Dev: 0.08143204661906174\n",
            "Epoch 426/800, Loss: 1.060198426246643, Std Dev: 0.08137127580159334\n",
            "Epoch 427/800, Loss: 1.0592337846755981, Std Dev: 0.08131195586876723\n",
            "Epoch 428/800, Loss: 1.0676648616790771, Std Dev: 0.0812416401580335\n",
            "Epoch 429/800, Loss: 1.069560170173645, Std Dev: 0.08116927900527761\n",
            "Epoch 430/800, Loss: 1.0567972660064697, Std Dev: 0.0811138130586785\n",
            "Epoch 431/800, Loss: 1.0693329572677612, Std Dev: 0.08104198702277199\n",
            "Epoch 432/800, Loss: 1.0599167346954346, Std Dev: 0.08098218466716135\n",
            "Epoch 433/800, Loss: 1.062583327293396, Std Dev: 0.08091885744875624\n",
            "Epoch 434/800, Loss: 1.0607081651687622, Std Dev: 0.08085813477142662\n",
            "Epoch 435/800, Loss: 1.0662999153137207, Std Dev: 0.08079036208089947\n",
            "Epoch 436/800, Loss: 1.0630607604980469, Std Dev: 0.08072673823542195\n",
            "Epoch 437/800, Loss: 1.0739444494247437, Std Dev: 0.08065097823152148\n",
            "Epoch 438/800, Loss: 1.068048357963562, Std Dev: 0.08058161343637169\n",
            "Epoch 439/800, Loss: 1.0675157308578491, Std Dev: 0.08051300764688926\n",
            "Epoch 440/800, Loss: 1.0713090896606445, Std Dev: 0.08044042556419166\n",
            "Epoch 441/800, Loss: 1.0662741661071777, Std Dev: 0.08037356494795456\n",
            "Epoch 442/800, Loss: 1.0553207397460938, Std Dev: 0.08032131400014096\n",
            "Epoch 443/800, Loss: 1.0629804134368896, Std Dev: 0.08025865807954045\n",
            "Epoch 444/800, Loss: 1.066245675086975, Std Dev: 0.08019217745986848\n",
            "Epoch 445/800, Loss: 1.062747836112976, Std Dev: 0.0801300419003002\n",
            "Epoch 446/800, Loss: 1.0635648965835571, Std Dev: 0.08006700172449606\n",
            "Epoch 447/800, Loss: 1.064281702041626, Std Dev: 0.08000320693942246\n",
            "Epoch 448/800, Loss: 1.0644135475158691, Std Dev: 0.07993937390796572\n",
            "Epoch 449/800, Loss: 1.0728087425231934, Std Dev: 0.0798666700022729\n",
            "Epoch 450/800, Loss: 1.0653367042541504, Std Dev: 0.0798020310888544\n",
            "Epoch 451/800, Loss: 1.0568369626998901, Std Dev: 0.07974833169715512\n",
            "Epoch 452/800, Loss: 1.057177186012268, Std Dev: 0.079694210681473\n",
            "Epoch 453/800, Loss: 1.0670944452285767, Std Dev: 0.0796279133539453\n",
            "Epoch 454/800, Loss: 1.0641076564788818, Std Dev: 0.07956514253259227\n",
            "Epoch 455/800, Loss: 1.0620299577713013, Std Dev: 0.07950497974184714\n",
            "Epoch 456/800, Loss: 1.0721077919006348, Std Dev: 0.07943398786603688\n",
            "Epoch 457/800, Loss: 1.0575007200241089, Std Dev: 0.07937987073149387\n",
            "Epoch 458/800, Loss: 1.0622245073318481, Std Dev: 0.07931979127626528\n",
            "Epoch 459/800, Loss: 1.0594137907028198, Std Dev: 0.07926330762136398\n",
            "Epoch 460/800, Loss: 1.0620572566986084, Std Dev: 0.07920361844287185\n",
            "Epoch 461/800, Loss: 1.0709151029586792, Std Dev: 0.07913446551558914\n",
            "Epoch 462/800, Loss: 1.065602421760559, Std Dev: 0.07907093942260805\n",
            "Epoch 463/800, Loss: 1.0560519695281982, Std Dev: 0.07901925583036212\n",
            "Epoch 464/800, Loss: 1.0681252479553223, Std Dev: 0.07895327932378858\n",
            "Epoch 465/800, Loss: 1.0721564292907715, Std Dev: 0.07888355986585392\n",
            "Epoch 466/800, Loss: 1.0628677606582642, Std Dev: 0.07882360856791504\n",
            "Epoch 467/800, Loss: 1.0677248239517212, Std Dev: 0.07875847258249795\n",
            "Epoch 468/800, Loss: 1.0639991760253906, Std Dev: 0.0786974664774544\n",
            "Epoch 469/800, Loss: 1.058505654335022, Std Dev: 0.07864311799039699\n",
            "Epoch 470/800, Loss: 1.0661696195602417, Std Dev: 0.07857996763901128\n",
            "Epoch 471/800, Loss: 1.0656737089157104, Std Dev: 0.078517471967542\n",
            "Epoch 472/800, Loss: 1.055376648902893, Std Dev: 0.07846744531603074\n",
            "Epoch 473/800, Loss: 1.0675475597381592, Std Dev: 0.07840323249282244\n",
            "Epoch 474/800, Loss: 1.069007396697998, Std Dev: 0.07833772537550868\n",
            "Epoch 475/800, Loss: 1.0617101192474365, Std Dev: 0.07828008176742374\n",
            "Epoch 476/800, Loss: 1.0573029518127441, Std Dev: 0.07822786687295069\n",
            "Epoch 477/800, Loss: 1.0632189512252808, Std Dev: 0.07816869693777323\n",
            "Epoch 478/800, Loss: 1.059812068939209, Std Dev: 0.07811354595084044\n",
            "Epoch 479/800, Loss: 1.0714442729949951, Std Dev: 0.07804644395589104\n",
            "Epoch 480/800, Loss: 1.0550141334533691, Std Dev: 0.07799749713698535\n",
            "Epoch 481/800, Loss: 1.063789963722229, Std Dev: 0.07793812248634036\n",
            "Epoch 482/800, Loss: 1.056311011314392, Std Dev: 0.0778876215350898\n",
            "Epoch 483/800, Loss: 1.060312032699585, Std Dev: 0.07783231648064146\n",
            "Epoch 484/800, Loss: 1.0572493076324463, Std Dev: 0.07778076223023705\n",
            "Epoch 485/800, Loss: 1.0649219751358032, Std Dev: 0.07772058393360363\n",
            "Epoch 486/800, Loss: 1.0657259225845337, Std Dev: 0.0776597075738749\n",
            "Epoch 487/800, Loss: 1.0666950941085815, Std Dev: 0.0775979963800941\n",
            "Epoch 488/800, Loss: 1.062682867050171, Std Dev: 0.07754053317436839\n",
            "Epoch 489/800, Loss: 1.0604557991027832, Std Dev: 0.07748563311990041\n",
            "Epoch 490/800, Loss: 1.0594706535339355, Std Dev: 0.07743194525738806\n",
            "Epoch 491/800, Loss: 1.064587116241455, Std Dev: 0.0773727832432958\n",
            "Epoch 492/800, Loss: 1.0632449388504028, Std Dev: 0.07731512214691308\n",
            "Epoch 493/800, Loss: 1.0684199333190918, Std Dev: 0.0772525026325088\n",
            "Epoch 494/800, Loss: 1.0672987699508667, Std Dev: 0.07719105649868328\n",
            "Epoch 495/800, Loss: 1.0575242042541504, Std Dev: 0.07714012174900009\n",
            "Epoch 496/800, Loss: 1.0559039115905762, Std Dev: 0.07709120508171245\n",
            "Epoch 497/800, Loss: 1.0655115842819214, Std Dev: 0.07703178423437387\n",
            "Epoch 498/800, Loss: 1.0607857704162598, Std Dev: 0.07697735909850018\n",
            "Epoch 499/800, Loss: 1.0554999113082886, Std Dev: 0.07692914420058052\n",
            "Epoch 500/800, Loss: 1.0651535987854004, Std Dev: 0.0768703813630372\n",
            "Epoch 501/800, Loss: 1.0637625455856323, Std Dev: 0.07681310902742361\n",
            "Epoch 502/800, Loss: 1.0635223388671875, Std Dev: 0.07675618629085458\n",
            "Epoch 503/800, Loss: 1.0703704357147217, Std Dev: 0.07669309475935894\n",
            "Epoch 504/800, Loss: 1.0586140155792236, Std Dev: 0.07664163571772753\n",
            "Epoch 505/800, Loss: 1.0572574138641357, Std Dev: 0.07659179862778255\n",
            "Epoch 506/800, Loss: 1.0627837181091309, Std Dev: 0.07653602671947668\n",
            "Epoch 507/800, Loss: 1.0594366788864136, Std Dev: 0.07648388266283257\n",
            "Epoch 508/800, Loss: 1.0519177913665771, Std Dev: 0.0764407623809905\n",
            "Epoch 509/800, Loss: 1.0614287853240967, Std Dev: 0.07638662802502531\n",
            "Epoch 510/800, Loss: 1.0668967962265015, Std Dev: 0.07632731088300446\n",
            "Epoch 511/800, Loss: 1.057676911354065, Std Dev: 0.07627742899853196\n",
            "Epoch 512/800, Loss: 1.0631481409072876, Std Dev: 0.07622184504460226\n",
            "Epoch 513/800, Loss: 1.0667037963867188, Std Dev: 0.0761630365654749\n",
            "Epoch 514/800, Loss: 1.0605714321136475, Std Dev: 0.07611027381514329\n",
            "Epoch 515/800, Loss: 1.0672467947006226, Std Dev: 0.07605121768626451\n",
            "Epoch 516/800, Loss: 1.063373327255249, Std Dev: 0.07599583989730528\n",
            "Epoch 517/800, Loss: 1.065610408782959, Std Dev: 0.07593847364248467\n",
            "Epoch 518/800, Loss: 1.0645904541015625, Std Dev: 0.07588215841100848\n",
            "Epoch 519/800, Loss: 1.0601924657821655, Std Dev: 0.07583027408207464\n",
            "Epoch 520/800, Loss: 1.0587565898895264, Std Dev: 0.07577998449949744\n",
            "Epoch 521/800, Loss: 1.0546965599060059, Std Dev: 0.07573431193168441\n",
            "Epoch 522/800, Loss: 1.0660126209259033, Std Dev: 0.07567709686681887\n",
            "Epoch 523/800, Loss: 1.0657614469528198, Std Dev: 0.07562022028949808\n",
            "Epoch 524/800, Loss: 1.0660760402679443, Std Dev: 0.07556318291384802\n",
            "Epoch 525/800, Loss: 1.0528111457824707, Std Dev: 0.07552003452232088\n",
            "Epoch 526/800, Loss: 1.059537410736084, Std Dev: 0.07546940473602057\n",
            "Epoch 527/800, Loss: 1.0523000955581665, Std Dev: 0.07542695740863964\n",
            "Epoch 528/800, Loss: 1.067237138748169, Std Dev: 0.07536930790989554\n",
            "Epoch 529/800, Loss: 1.0607659816741943, Std Dev: 0.07531767446178546\n",
            "Epoch 530/800, Loss: 1.0663397312164307, Std Dev: 0.07526100386505603\n",
            "Epoch 531/800, Loss: 1.0607997179031372, Std Dev: 0.07520952482461833\n",
            "Epoch 532/800, Loss: 1.0627092123031616, Std Dev: 0.07515630132200508\n",
            "Epoch 533/800, Loss: 1.0589617490768433, Std Dev: 0.07510684100168637\n",
            "Epoch 534/800, Loss: 1.0616413354873657, Std Dev: 0.07505480567828308\n",
            "Epoch 535/800, Loss: 1.066431999206543, Std Dev: 0.07499858359340979\n",
            "Epoch 536/800, Loss: 1.0616161823272705, Std Dev: 0.07494676599787864\n",
            "Epoch 537/800, Loss: 1.0622936487197876, Std Dev: 0.07489440195425853\n",
            "Epoch 538/800, Loss: 1.0605559349060059, Std Dev: 0.07484378108215939\n",
            "Epoch 539/800, Loss: 1.0643200874328613, Std Dev: 0.07478977531079224\n",
            "Epoch 540/800, Loss: 1.069379448890686, Std Dev: 0.07473177730999996\n",
            "Epoch 541/800, Loss: 1.066823124885559, Std Dev: 0.07467589219680514\n",
            "Epoch 542/800, Loss: 1.0619845390319824, Std Dev: 0.07462431008137964\n",
            "Epoch 543/800, Loss: 1.0562326908111572, Std Dev: 0.07457853099041364\n",
            "Epoch 544/800, Loss: 1.0658330917358398, Std Dev: 0.07452375857849915\n",
            "Epoch 545/800, Loss: 1.0594288110733032, Std Dev: 0.07447486528533492\n",
            "Epoch 546/800, Loss: 1.0626108646392822, Std Dev: 0.07442306467897597\n",
            "Epoch 547/800, Loss: 1.0647904872894287, Std Dev: 0.0743694634426478\n",
            "Epoch 548/800, Loss: 1.0577389001846313, Std Dev: 0.07432249788513981\n",
            "Epoch 549/800, Loss: 1.0605835914611816, Std Dev: 0.07427282498222532\n",
            "Epoch 550/800, Loss: 1.0616041421890259, Std Dev: 0.07422229043732737\n",
            "Epoch 551/800, Loss: 1.061793327331543, Std Dev: 0.07417167176029278\n",
            "Epoch 552/800, Loss: 1.0687278509140015, Std Dev: 0.07411547131841578\n",
            "Epoch 553/800, Loss: 1.0656952857971191, Std Dev: 0.07406172078068456\n",
            "Epoch 554/800, Loss: 1.0660786628723145, Std Dev: 0.07400777163378916\n",
            "Epoch 555/800, Loss: 1.0687918663024902, Std Dev: 0.07395187469042101\n",
            "Epoch 556/800, Loss: 1.066514253616333, Std Dev: 0.07389780749172997\n",
            "Epoch 557/800, Loss: 1.0632703304290771, Std Dev: 0.07384649535858454\n",
            "Epoch 558/800, Loss: 1.058998465538025, Std Dev: 0.07379913955751694\n",
            "Epoch 559/800, Loss: 1.06473708152771, Std Dev: 0.07374678548189066\n",
            "Epoch 560/800, Loss: 1.0578173398971558, Std Dev: 0.07370072839047687\n",
            "Epoch 561/800, Loss: 1.0577930212020874, Std Dev: 0.073654760035294\n",
            "Epoch 562/800, Loss: 1.0615798234939575, Std Dev: 0.07360534160650618\n",
            "Epoch 563/800, Loss: 1.0611575841903687, Std Dev: 0.07355638215640724\n",
            "Epoch 564/800, Loss: 1.0655717849731445, Std Dev: 0.0735038220996492\n",
            "Epoch 565/800, Loss: 1.0561726093292236, Std Dev: 0.07345975124409546\n",
            "Epoch 566/800, Loss: 1.0609737634658813, Std Dev: 0.07341120083129628\n",
            "Epoch 567/800, Loss: 1.0690256357192993, Std Dev: 0.07335638903902224\n",
            "Epoch 568/800, Loss: 1.0647261142730713, Std Dev: 0.07330487949294368\n",
            "Epoch 569/800, Loss: 1.0599009990692139, Std Dev: 0.07325755991386541\n",
            "Epoch 570/800, Loss: 1.0618125200271606, Std Dev: 0.07320863351866949\n",
            "Epoch 571/800, Loss: 1.054856538772583, Std Dev: 0.07316631642199593\n",
            "Epoch 572/800, Loss: 1.0694159269332886, Std Dev: 0.07311175359792352\n",
            "Epoch 573/800, Loss: 1.0577250719070435, Std Dev: 0.07306675576737907\n",
            "Epoch 574/800, Loss: 1.0621273517608643, Std Dev: 0.07301789323477585\n",
            "Epoch 575/800, Loss: 1.0623146295547485, Std Dev: 0.07296896020397732\n",
            "Epoch 576/800, Loss: 1.061759114265442, Std Dev: 0.07292057900822412\n",
            "Epoch 577/800, Loss: 1.057839274406433, Std Dev: 0.0728757630557127\n",
            "Epoch 578/800, Loss: 1.065792202949524, Std Dev: 0.07282434870402754\n",
            "Epoch 579/800, Loss: 1.0577118396759033, Std Dev: 0.07277979970727202\n",
            "Epoch 580/800, Loss: 1.0560715198516846, Std Dev: 0.0727368714605543\n",
            "Epoch 581/800, Loss: 1.0631020069122314, Std Dev: 0.07268778884001734\n",
            "Epoch 582/800, Loss: 1.0544875860214233, Std Dev: 0.07264654416123116\n",
            "Epoch 583/800, Loss: 1.0602362155914307, Std Dev: 0.0725999956061759\n",
            "Epoch 584/800, Loss: 1.0607120990753174, Std Dev: 0.07255311599418669\n",
            "Epoch 585/800, Loss: 1.0643200874328613, Std Dev: 0.07250341205871991\n",
            "Epoch 586/800, Loss: 1.0627532005310059, Std Dev: 0.07245502124802738\n",
            "Epoch 587/800, Loss: 1.0633893013000488, Std Dev: 0.07240621656561218\n",
            "Epoch 588/800, Loss: 1.063998818397522, Std Dev: 0.07235703167255596\n",
            "Epoch 589/800, Loss: 1.0603997707366943, Std Dev: 0.0723108292843617\n",
            "Epoch 590/800, Loss: 1.0593304634094238, Std Dev: 0.07226561738231428\n",
            "Epoch 591/800, Loss: 1.060322642326355, Std Dev: 0.07221962849528464\n",
            "Epoch 592/800, Loss: 1.0621118545532227, Std Dev: 0.07217224870461296\n",
            "Epoch 593/800, Loss: 1.0659263134002686, Std Dev: 0.07212208431336462\n",
            "Epoch 594/800, Loss: 1.0652028322219849, Std Dev: 0.07207253445150794\n",
            "Epoch 595/800, Loss: 1.0592776536941528, Std Dev: 0.07202775623070784\n",
            "Epoch 596/800, Loss: 1.059719443321228, Std Dev: 0.07198267169594784\n",
            "Epoch 597/800, Loss: 1.0533195734024048, Std Dev: 0.07194353167538825\n",
            "Epoch 598/800, Loss: 1.0622478723526, Std Dev: 0.07189652336004461\n",
            "Epoch 599/800, Loss: 1.0547864437103271, Std Dev: 0.07185606230764882\n",
            "Epoch 600/800, Loss: 1.0538703203201294, Std Dev: 0.07181652856290231\n",
            "Epoch 601/800, Loss: 1.0732011795043945, Std Dev: 0.07176263178899174\n",
            "Epoch 602/800, Loss: 1.0679289102554321, Std Dev: 0.07171191823205435\n",
            "Epoch 603/800, Loss: 1.0576128959655762, Std Dev: 0.07166914313301818\n",
            "Epoch 604/800, Loss: 1.058078408241272, Std Dev: 0.07162602356373754\n",
            "Epoch 605/800, Loss: 1.0532939434051514, Std Dev: 0.07158736074564184\n",
            "Epoch 606/800, Loss: 1.0583858489990234, Std Dev: 0.07154409237885219\n",
            "Epoch 607/800, Loss: 1.0615290403366089, Std Dev: 0.07149832635820205\n",
            "Epoch 608/800, Loss: 1.0617753267288208, Std Dev: 0.07145244772264163\n",
            "Epoch 609/800, Loss: 1.064726710319519, Std Dev: 0.07140447763292541\n",
            "Epoch 610/800, Loss: 1.058335781097412, Std Dev: 0.07136153735804737\n",
            "Epoch 611/800, Loss: 1.0568156242370605, Std Dev: 0.07131997023207028\n",
            "Epoch 612/800, Loss: 1.056157112121582, Std Dev: 0.0712790418775703\n",
            "Epoch 613/800, Loss: 1.056281328201294, Std Dev: 0.07123805681657772\n",
            "Epoch 614/800, Loss: 1.0635855197906494, Std Dev: 0.07119127918333745\n",
            "Epoch 615/800, Loss: 1.0625272989273071, Std Dev: 0.07114535612436186\n",
            "Epoch 616/800, Loss: 1.0617921352386475, Std Dev: 0.0711000620867577\n",
            "Epoch 617/800, Loss: 1.0621012449264526, Std Dev: 0.07105461430433703\n",
            "Epoch 618/800, Loss: 1.0657607316970825, Std Dev: 0.0710066776202847\n",
            "Epoch 619/800, Loss: 1.057550072669983, Std Dev: 0.07096500391236288\n",
            "Epoch 620/800, Loss: 1.0677932500839233, Std Dev: 0.07091594892599815\n",
            "Epoch 621/800, Loss: 1.0711132287979126, Std Dev: 0.07086509619762095\n",
            "Epoch 622/800, Loss: 1.0683319568634033, Std Dev: 0.0708159177102045\n",
            "Epoch 623/800, Loss: 1.0561034679412842, Std Dev: 0.07077578828115193\n",
            "Epoch 624/800, Loss: 1.059389591217041, Std Dev: 0.07073298092174013\n",
            "Epoch 625/800, Loss: 1.059374213218689, Std Dev: 0.07069025240768116\n",
            "Epoch 626/800, Loss: 1.0568114519119263, Std Dev: 0.07064968896005962\n",
            "Epoch 627/800, Loss: 1.0587100982666016, Std Dev: 0.07060761617831773\n",
            "Epoch 628/800, Loss: 1.0580750703811646, Std Dev: 0.07056611996592323\n",
            "Epoch 629/800, Loss: 1.06015944480896, Std Dev: 0.07052303910139704\n",
            "Epoch 630/800, Loss: 1.0714606046676636, Std Dev: 0.07047282865573784\n",
            "Epoch 631/800, Loss: 1.0562400817871094, Std Dev: 0.07043306797676649\n",
            "Epoch 632/800, Loss: 1.060930609703064, Std Dev: 0.07038962744148788\n",
            "Epoch 633/800, Loss: 1.0621602535247803, Std Dev: 0.07034536411910558\n",
            "Epoch 634/800, Loss: 1.058975100517273, Std Dev: 0.070303557898296\n",
            "Epoch 635/800, Loss: 1.0630041360855103, Std Dev: 0.07025884963378669\n",
            "Epoch 636/800, Loss: 1.0541554689407349, Std Dev: 0.07022119467484106\n",
            "Epoch 637/800, Loss: 1.0692658424377441, Std Dev: 0.07017276218633368\n",
            "Epoch 638/800, Loss: 1.0600426197052002, Std Dev: 0.07013041258032415\n",
            "Epoch 639/800, Loss: 1.0635342597961426, Std Dev: 0.07008564783758235\n",
            "Epoch 640/800, Loss: 1.0625073909759521, Std Dev: 0.07004166306834816\n",
            "Epoch 641/800, Loss: 1.0610675811767578, Std Dev: 0.06999877318476841\n",
            "Epoch 642/800, Loss: 1.0592031478881836, Std Dev: 0.06995733848284483\n",
            "Epoch 643/800, Loss: 1.0660321712493896, Std Dev: 0.0699112862652625\n",
            "Epoch 644/800, Loss: 1.0686333179473877, Std Dev: 0.06986381397004056\n",
            "Epoch 645/800, Loss: 1.0584983825683594, Std Dev: 0.06982313950416937\n",
            "Epoch 646/800, Loss: 1.0565372705459595, Std Dev: 0.06978408183111154\n",
            "Epoch 647/800, Loss: 1.067138671875, Std Dev: 0.0697376988392186\n",
            "Epoch 648/800, Loss: 1.0627576112747192, Std Dev: 0.0696941461618724\n",
            "Epoch 649/800, Loss: 1.0537233352661133, Std Dev: 0.06965764516767829\n",
            "Epoch 650/800, Loss: 1.0657851696014404, Std Dev: 0.06961229883297092\n",
            "Epoch 651/800, Loss: 1.0571250915527344, Std Dev: 0.06957307846603603\n",
            "Epoch 652/800, Loss: 1.0549540519714355, Std Dev: 0.06953568197078895\n",
            "Epoch 653/800, Loss: 1.0631753206253052, Std Dev: 0.06949219535189927\n",
            "Epoch 654/800, Loss: 1.0599783658981323, Std Dev: 0.06945098887083546\n",
            "Epoch 655/800, Loss: 1.062242865562439, Std Dev: 0.06940826696472788\n",
            "Epoch 656/800, Loss: 1.0573437213897705, Std Dev: 0.06936917039670182\n",
            "Epoch 657/800, Loss: 1.060039758682251, Std Dev: 0.0693281137673184\n",
            "Epoch 658/800, Loss: 1.065819263458252, Std Dev: 0.06928334881791937\n",
            "Epoch 659/800, Loss: 1.0564544200897217, Std Dev: 0.0692451342997255\n",
            "Epoch 660/800, Loss: 1.0601102113723755, Std Dev: 0.06920422511624338\n",
            "Epoch 661/800, Loss: 1.0616207122802734, Std Dev: 0.0691623339399543\n",
            "Epoch 662/800, Loss: 1.0649102926254272, Std Dev: 0.06911841054069925\n",
            "Epoch 663/800, Loss: 1.0609430074691772, Std Dev: 0.06907712510933331\n",
            "Epoch 664/800, Loss: 1.0649998188018799, Std Dev: 0.06903330118957614\n",
            "Epoch 665/800, Loss: 1.0622574090957642, Std Dev: 0.06899127485283686\n",
            "Epoch 666/800, Loss: 1.0572572946548462, Std Dev: 0.06895286522805415\n",
            "Epoch 667/800, Loss: 1.054356336593628, Std Dev: 0.06891681004798016\n",
            "Epoch 668/800, Loss: 1.0638470649719238, Std Dev: 0.0688739675879056\n",
            "Epoch 669/800, Loss: 1.0506147146224976, Std Dev: 0.06884123865211837\n",
            "Epoch 670/800, Loss: 1.0595793724060059, Std Dev: 0.06880134009426382\n",
            "Epoch 671/800, Loss: 1.0633172988891602, Std Dev: 0.06875902301687185\n",
            "Epoch 672/800, Loss: 1.0658034086227417, Std Dev: 0.06871530069444133\n",
            "Epoch 673/800, Loss: 1.058544397354126, Std Dev: 0.06867634000198226\n",
            "Epoch 674/800, Loss: 1.055801510810852, Std Dev: 0.0686394973875476\n",
            "Epoch 675/800, Loss: 1.0645719766616821, Std Dev: 0.0685967004649884\n",
            "Epoch 676/800, Loss: 1.0607682466506958, Std Dev: 0.06855637375600333\n",
            "Epoch 677/800, Loss: 1.0658234357833862, Std Dev: 0.0685130059878485\n",
            "Epoch 678/800, Loss: 1.0677214860916138, Std Dev: 0.06846869567343\n",
            "Epoch 679/800, Loss: 1.0647729635238647, Std Dev: 0.06842608577668086\n",
            "Epoch 680/800, Loss: 1.0605522394180298, Std Dev: 0.06838618543776238\n",
            "Epoch 681/800, Loss: 1.0548324584960938, Std Dev: 0.06835051846257478\n",
            "Epoch 682/800, Loss: 1.0645825862884521, Std Dev: 0.0683082256032207\n",
            "Epoch 683/800, Loss: 1.0639841556549072, Std Dev: 0.06826635743656281\n",
            "Epoch 684/800, Loss: 1.0560601949691772, Std Dev: 0.06822990636330617\n",
            "Epoch 685/800, Loss: 1.058773159980774, Std Dev: 0.0681915269948689\n",
            "Epoch 686/800, Loss: 1.0561717748641968, Std Dev: 0.06815509199537992\n",
            "Epoch 687/800, Loss: 1.0627585649490356, Std Dev: 0.068114217881032\n",
            "Epoch 688/800, Loss: 1.06308114528656, Std Dev: 0.06807321722018995\n",
            "Epoch 689/800, Loss: 1.064958095550537, Std Dev: 0.06803119515922279\n",
            "Epoch 690/800, Loss: 1.0571573972702026, Std Dev: 0.06799426384226712\n",
            "Epoch 691/800, Loss: 1.0593745708465576, Std Dev: 0.06795582990674576\n",
            "Epoch 692/800, Loss: 1.059944748878479, Std Dev: 0.06791707248576154\n",
            "Epoch 693/800, Loss: 1.0615850687026978, Std Dev: 0.06787731770471857\n",
            "Epoch 694/800, Loss: 1.061920404434204, Std Dev: 0.06783741898161998\n",
            "Epoch 695/800, Loss: 1.0601102113723755, Std Dev: 0.06779873771993569\n",
            "Epoch 696/800, Loss: 1.0673421621322632, Std Dev: 0.06775594009251727\n",
            "Epoch 697/800, Loss: 1.0653403997421265, Std Dev: 0.06771426426001363\n",
            "Epoch 698/800, Loss: 1.0564510822296143, Std Dev: 0.06767830608190026\n",
            "Epoch 699/800, Loss: 1.0574917793273926, Std Dev: 0.06764165056820046\n",
            "Epoch 700/800, Loss: 1.0613830089569092, Std Dev: 0.06760246858077253\n",
            "Epoch 701/800, Loss: 1.060351848602295, Std Dev: 0.06756400020932947\n",
            "Epoch 702/800, Loss: 1.0613203048706055, Std Dev: 0.0675249816020477\n",
            "Epoch 703/800, Loss: 1.0612646341323853, Std Dev: 0.06748606007614043\n",
            "Epoch 704/800, Loss: 1.0580501556396484, Std Dev: 0.06744929683683366\n",
            "Epoch 705/800, Loss: 1.0634288787841797, Std Dev: 0.06740920964549899\n",
            "Epoch 706/800, Loss: 1.063806176185608, Std Dev: 0.06736897671927357\n",
            "Epoch 707/800, Loss: 1.0691139698028564, Std Dev: 0.06732613113579057\n",
            "Epoch 708/800, Loss: 1.0574132204055786, Std Dev: 0.06729004832340153\n",
            "Epoch 709/800, Loss: 1.060230016708374, Std Dev: 0.06725214912839557\n",
            "Epoch 710/800, Loss: 1.0631985664367676, Std Dev: 0.06721252532299962\n",
            "Epoch 711/800, Loss: 1.0620001554489136, Std Dev: 0.06717366405410544\n",
            "Epoch 712/800, Loss: 1.0682176351547241, Std Dev: 0.06713158831779684\n",
            "Epoch 713/800, Loss: 1.0648167133331299, Std Dev: 0.06709127926180032\n",
            "Epoch 714/800, Loss: 1.069167971611023, Std Dev: 0.06704892678770469\n",
            "Epoch 715/800, Loss: 1.0578957796096802, Std Dev: 0.06701292922250779\n",
            "Epoch 716/800, Loss: 1.059478998184204, Std Dev: 0.06697594357710891\n",
            "Epoch 717/800, Loss: 1.060076355934143, Std Dev: 0.06693863577744208\n",
            "Epoch 718/800, Loss: 1.0607296228408813, Std Dev: 0.06690098146203004\n",
            "Epoch 719/800, Loss: 1.0633445978164673, Std Dev: 0.06686186286650428\n",
            "Epoch 720/800, Loss: 1.0547497272491455, Std Dev: 0.06682833773156044\n",
            "Epoch 721/800, Loss: 1.0600674152374268, Std Dev: 0.06679126049349804\n",
            "Epoch 722/800, Loss: 1.0611125230789185, Std Dev: 0.06675360406013671\n",
            "Epoch 723/800, Loss: 1.0581673383712769, Std Dev: 0.06671785228212238\n",
            "Epoch 724/800, Loss: 1.059523582458496, Std Dev: 0.06668128123647976\n",
            "Epoch 725/800, Loss: 1.056198239326477, Std Dev: 0.06664696136572143\n",
            "Epoch 726/800, Loss: 1.0515923500061035, Std Dev: 0.06661609863649524\n",
            "Epoch 727/800, Loss: 1.0651124715805054, Std Dev: 0.06657651518903566\n",
            "Epoch 728/800, Loss: 1.0579921007156372, Std Dev: 0.0665411299669803\n",
            "Epoch 729/800, Loss: 1.0550143718719482, Std Dev: 0.06650782735694374\n",
            "Epoch 730/800, Loss: 1.0628031492233276, Std Dev: 0.06646964646052328\n",
            "Epoch 731/800, Loss: 1.065237045288086, Std Dev: 0.06643024963364666\n",
            "Epoch 732/800, Loss: 1.0670663118362427, Std Dev: 0.06639004188425791\n",
            "Epoch 733/800, Loss: 1.0660276412963867, Std Dev: 0.06635039489884721\n",
            "Epoch 734/800, Loss: 1.0637426376342773, Std Dev: 0.06631196927980311\n",
            "Epoch 735/800, Loss: 1.0570110082626343, Std Dev: 0.06627761691937487\n",
            "Epoch 736/800, Loss: 1.058894157409668, Std Dev: 0.06624209847661935\n",
            "Epoch 737/800, Loss: 1.0628197193145752, Std Dev: 0.06620434288288268\n",
            "Epoch 738/800, Loss: 1.0534040927886963, Std Dev: 0.06617265546947491\n",
            "Epoch 739/800, Loss: 1.0570733547210693, Std Dev: 0.06613845255635895\n",
            "Epoch 740/800, Loss: 1.0630486011505127, Std Dev: 0.06610073933751229\n",
            "Epoch 741/800, Loss: 1.0604660511016846, Std Dev: 0.06606453237609482\n",
            "Epoch 742/800, Loss: 1.0618329048156738, Std Dev: 0.06602760068110403\n",
            "Epoch 743/800, Loss: 1.057892918586731, Std Dev: 0.06599307395132414\n",
            "Epoch 744/800, Loss: 1.0646065473556519, Std Dev: 0.06595479533179338\n",
            "Epoch 745/800, Loss: 1.062551736831665, Std Dev: 0.06591764450177959\n",
            "Epoch 746/800, Loss: 1.0610374212265015, Std Dev: 0.06588139026177829\n",
            "Epoch 747/800, Loss: 1.057629942893982, Std Dev: 0.06584724138828003\n",
            "Epoch 748/800, Loss: 1.0657926797866821, Std Dev: 0.06580863468406876\n",
            "Epoch 749/800, Loss: 1.0632727146148682, Std Dev: 0.06577133803421403\n",
            "Epoch 750/800, Loss: 1.0639415979385376, Std Dev: 0.06573376002540605\n",
            "Epoch 751/800, Loss: 1.0576757192611694, Std Dev: 0.06569979612433431\n",
            "Epoch 752/800, Loss: 1.0678377151489258, Std Dev: 0.06566053157405627\n",
            "Epoch 753/800, Loss: 1.0621799230575562, Std Dev: 0.06562405054908245\n",
            "Epoch 754/800, Loss: 1.0679162740707397, Std Dev: 0.06558488583833753\n",
            "Epoch 755/800, Loss: 1.0581369400024414, Std Dev: 0.06555085344750285\n",
            "Epoch 756/800, Loss: 1.0598511695861816, Std Dev: 0.06551584377625867\n",
            "Epoch 757/800, Loss: 1.0696910619735718, Std Dev: 0.06547616055067536\n",
            "Epoch 758/800, Loss: 1.0600579977035522, Std Dev: 0.0654411463781426\n",
            "Epoch 759/800, Loss: 1.0657235383987427, Std Dev: 0.06540326043144316\n",
            "Epoch 760/800, Loss: 1.0621275901794434, Std Dev: 0.0653672159639957\n",
            "Epoch 761/800, Loss: 1.0511889457702637, Std Dev: 0.06533821949400201\n",
            "Epoch 762/800, Loss: 1.057871699333191, Std Dev: 0.06530470336568384\n",
            "Epoch 763/800, Loss: 1.0585594177246094, Std Dev: 0.06527081727855454\n",
            "Epoch 764/800, Loss: 1.0669264793395996, Std Dev: 0.06523268567726347\n",
            "Epoch 765/800, Loss: 1.0669816732406616, Std Dev: 0.06519459592033247\n",
            "Epoch 766/800, Loss: 1.0581353902816772, Std Dev: 0.06516112697463868\n",
            "Epoch 767/800, Loss: 1.058634877204895, Std Dev: 0.06512740617540644\n",
            "Epoch 768/800, Loss: 1.0688414573669434, Std Dev: 0.06508874170248362\n",
            "Epoch 769/800, Loss: 1.0561977624893188, Std Dev: 0.06505662500027341\n",
            "Epoch 770/800, Loss: 1.0589898824691772, Std Dev: 0.0650228476340608\n",
            "Epoch 771/800, Loss: 1.0672844648361206, Std Dev: 0.06498498939977707\n",
            "Epoch 772/800, Loss: 1.0588006973266602, Std Dev: 0.06495142767194538\n",
            "Epoch 773/800, Loss: 1.0628248453140259, Std Dev: 0.06491573226350997\n",
            "Epoch 774/800, Loss: 1.0617254972457886, Std Dev: 0.06488065592635399\n",
            "Epoch 775/800, Loss: 1.059455394744873, Std Dev: 0.06484686910968009\n",
            "Epoch 776/800, Loss: 1.062721610069275, Std Dev: 0.06481138924527613\n",
            "Epoch 777/800, Loss: 1.0617789030075073, Std Dev: 0.06477644510924468\n",
            "Epoch 778/800, Loss: 1.0633840560913086, Std Dev: 0.06474074952025727\n",
            "Epoch 779/800, Loss: 1.0595358610153198, Std Dev: 0.06470712250821078\n",
            "Epoch 780/800, Loss: 1.060855507850647, Std Dev: 0.0646728225091901\n",
            "Epoch 781/800, Loss: 1.0595508813858032, Std Dev: 0.06463928543051485\n",
            "Epoch 782/800, Loss: 1.0541443824768066, Std Dev: 0.06460909808972493\n",
            "Epoch 783/800, Loss: 1.0483601093292236, Std Dev: 0.06458310872083287\n",
            "Epoch 784/800, Loss: 1.061470627784729, Std Dev: 0.06454866827054927\n",
            "Epoch 785/800, Loss: 1.0522122383117676, Std Dev: 0.06451989794760374\n",
            "Epoch 786/800, Loss: 1.0600496530532837, Std Dev: 0.0644863021616556\n",
            "Epoch 787/800, Loss: 1.06392240524292, Std Dev: 0.06445080740136128\n",
            "Epoch 788/800, Loss: 1.0633456707000732, Std Dev: 0.06441564017348456\n",
            "Epoch 789/800, Loss: 1.0648834705352783, Std Dev: 0.0643798222027307\n",
            "Epoch 790/800, Loss: 1.06177818775177, Std Dev: 0.06434553345761503\n",
            "Epoch 791/800, Loss: 1.062268614768982, Std Dev: 0.0643110524096995\n",
            "Epoch 792/800, Loss: 1.065016508102417, Std Dev: 0.06427534472427388\n",
            "Epoch 793/800, Loss: 1.0638999938964844, Std Dev: 0.06424019605629384\n",
            "Epoch 794/800, Loss: 1.0527050495147705, Std Dev: 0.06421146722048393\n",
            "Epoch 795/800, Loss: 1.0650829076766968, Std Dev: 0.06417589139276858\n",
            "Epoch 796/800, Loss: 1.061922311782837, Std Dev: 0.06414184134933065\n",
            "Epoch 797/800, Loss: 1.0579512119293213, Std Dev: 0.06410996059363816\n",
            "Epoch 798/800, Loss: 1.0626220703125, Std Dev: 0.06407566938061983\n",
            "Epoch 799/800, Loss: 1.0599884986877441, Std Dev: 0.06404275946547254\n",
            "Epoch 800/800, Loss: 1.0633975267410278, Std Dev: 0.06400820662041294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_max = torch.max(y.cpu()).item()\n",
        "y_min = torch.min(y.cpu()).item()\n",
        "alpha = y_max - y_min\n",
        "\n",
        "# Accuracy by error\n",
        "mae_accuracy = (1 - mae_score_value / alpha ) * 100\n",
        "rmse_accuracy = (1 - rmse_score_value / alpha) * 100\n",
        "mse_accuracy = (1 - mse_score_value / alpha) * 100\n",
        "\n",
        "# Calculate standard deviations based on test set\n",
        "mae_std = np.std([mean_absolute_error(y_test.cpu(), model(X_test).cpu().detach().numpy()) for _ in range(epochs)])\n",
        "rmse_std = np.std([np.sqrt(mean_squared_error(y_test.cpu(), model(X_test).cpu().detach().numpy())) for _ in range(epochs)])\n",
        "mse_std = np.std([mean_squared_error(y_test.cpu(), model(X_test).cpu().detach().numpy()) for _ in range(epochs)])\n",
        "\n",
        "print(f\"R^2 Score: {-r2_score_value}\")\n",
        "print(f\"MAE: {mae_score_value}\")\n",
        "print(f\"RMSE: {rmse_score_value}, Accuracy by error: {rmse_accuracy}%, Std Dev: ±{rmse_std}\")\n",
        "print(f\"MSE: {mse_score_value}, Accuracy by error: {mse_accuracy}%, Std Dev: ±{mse_std}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk4ZNCYUrKrE",
        "outputId": "c1c71e8a-691f-409f-e314-c6e31d04a149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 Score: 0.017728232785241937\n",
            "MAE: 0.7584974765777588\n",
            "RMSE: 0.9992857575416565, Accuracy by error: 87.38510028965484%, Std Dev: ±0.0\n",
            "MSE: 0.998572051525116, Accuracy by error: 87.39411005463286%, Std Dev: ±5.960464477539063e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy by Error: MAE is :\",mae_accuracy,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlAORyXIJt4E",
        "outputId": "ba1fe2b4-8353-4ff9-bb97-4db3cba36e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy by Error: MAE is : 90.42479138187913 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "metrics = ['MAE', 'RMSE', 'MSE']\n",
        "accuracies = [90.42479138187913, 87.38510028965484, 87.39411005463286]\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Create bar chart\n",
        "index = np.arange(len(metrics))\n",
        "bar_width = 0.5\n",
        "\n",
        "bars = ax.bar(index, accuracies, bar_width, color='g', alpha=0.7)\n",
        "\n",
        "# Labels and title\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_xticks(index)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.set_title('Accuracy by Error Metrics')\n",
        "\n",
        "# Adding value labels on bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}%', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "cykeHo4ArRp6",
        "outputId": "5e7aa241-2bfe-4028-b934-a2caea7caf89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}